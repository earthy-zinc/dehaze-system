# 课堂汇报

由于基于深度学习的图像去雾算法很多都是基于卷积神经网络架构的，卷积神经网络是专门为处理图像数据而设计的网络。了解卷积神经网络的基本组成有助于我们更好的理解计算机视觉前沿论文的算法，由于时间有限，可能有些同学对深度学习了解不多。因此本次汇报我就主要讲解深度学习的基础知识，同时讲解一个基于卷积神经网络的网络模型——残差网络ResNet。

# 1. 深度学习基础知识

## 1.1 提出问题

假设我们需要计算机帮我们识别出一张照片中是否存在小狗，我们需要怎么做呢？

在深度学习中，我们首先要精确的定义问题：这张照片中是否存在小狗，这是一个目标分类问题。

我们给定一张照片作为输入，通过一个神经网络模型的计算，计算机会在 {是, 否} 中选择一个作为输出。

那么这个网络模型是解决这个问题的重点，刚开始模型本身并不知道照片中是否存在有小狗，我们需要设计这个模型，并使用大量的数据训练，检查模型的表现，然后优化模型，重复训练过程，直到训练结果让人满意为止。

## 1.2 问题思路

提供给深度学习模型训练的这些数据称为数据集。为了训练一个模型而收集的数据称为训练集，为了测试这个模型是否在新数据集下是否有效而收集的数据称为测试集。

上述模型中的数据集中的数据，每个数据本身都带有特征，这些特征可以帮助模型进行预测。每个数据也都带有标签，也就是我们事先已经知道照片中是否存在小猫，并写入到标签中。

标签的值这是图片的真实情况，需要跟模型预测出来的结果做出区分。

这种带有 特征和标签 的数据被称为一个样本。我们把一组样本输入模型进行训练，这样的过程被称为监督学习。

## 1.4 线性回归模型

### 1.4.1 目标函数

模型里面最重要的部分之一就是目标函数，我们用目标函数来判断模型的效果。但是一开始并不存在一个完美的目标函数，我们需要通过训练来调整目标函数的参数，并且希望目标函数预测的结果和我们希望的结果尽可能的一致，这时我们就需要定义一个损失函数，来判断预测值和真实值之间的差异。损失函数越大，说明模型预测的结果和真实的情况差异越大。因此我们需要一种算法，能够对目标函数做出一些调整，从而最小化损失函数，让模型预测的结果和真实的情况尽可能的接近。这种算法就叫做优化算法。目前最流行的优化算法为梯度下降算法——gradient descent。

给定一个数据集，x 表示输入的特征，y 表示预测的结果，我们可以把线性回归模型中的目标函数定义为如下表示：、

ω 称为权重（weight），权重决定了每个特征对我们预测值的影响。 b 称为偏置（bias）、偏移量（offset）或截距（intercept）。将所有特征放到向量 x 中，并将所有权重放到向量 ω 中， 我们可以用点积形式来简洁地表达模型：

一开始，我们可以把目标函数定的简单一点，我们就使用这样一个线性函数作为目标模型，它需要d个特征，特征是从数据集中抽取出来的。我们通过这样一个函数可以获得模型预测的结果y。但是我们并不知道模型中的权重和偏置应该取多少才能够使模型的预测效果达到最好，因此我们需要两个东西： （1）一种模型质量的度量方式； （2）一种能够更新模型以提高模型预测质量的方法。

### 1.4.2 损失函数

损失函数（loss function）能够量化目标的实际值与预测值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 回归问题中最常用的损失函数是平方误差函数。 当样本i的预测值为y^(i)，其相应的真实标签为y(i)时， 平方误差可以定义为以下公式：

为了度量模型在整个数据集上的质量，我们需计算在训练集个样本上的损失均值（也等价于求和）。然后我们寻找一组参数——对应目标函数中的权重和偏置，这组参数能够最小化所有训练样本上的总损失。

## 1.4 softmax回归

在线性回归的目标函数中，预测的结果y用于预测多少的问题，比如说我们可以用预测结果y表示预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。但是它并不能分类问题，比如

- 某个电子邮件是否属于垃圾邮件文件夹？
- 某个用户可能*注册*或*不注册*订阅服务？
- 某个图像描绘的是驴、狗、猫、还是鸡？
- 某人接下来最有可能看哪部电影？

因此我们如果想要将线性回归模型用于分类问题，需要将预测的结果转换成预测分类问题中的概率，进而选择预测概率最大的哪一个作为预测结果。

那么softmax是如何实现预测分类问题的呢？

我们如果想要机器处理分类问题中的类别，计算机擅长处理0 1 这些数据，一个比较好的方法就是用one-hot encoding，设置一个向量，分量跟类别数一样多，类别对应的分量设为1，其他分量设为0。

输入n个特征，输出m个类别，输入层和输出层之间使用线性回归函数进行全连接，如下图所示。

为了将输出转化为概率，我们再对上层的输出使用softmax函数，保证最终输出的类别的概率值总和为1。

总结道，基础的线性回归模型可以用于预测数量的多少问题，但是不好预测分类问题。而softmax回归解决了线性模型中的分类问题。

## 1.5  激活函数

之前所说的模型都是基于目标函数是线性的假设，但是日常生活中很多情况并不是线性增加或者线性减少的。不能仅仅用线性函数就能够模拟出来，因此我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。

这些隐藏层是输入层和输出层之间的神经网络层，因此也能叫做中间层。但是单纯的靠堆叠网络层数不一定能够使线性函数变成非线性函数。

因此我们再中间层中需要再添加一层非线性的激活函数，激活函数是非线性的。因此添加了激活函数之后的网络就不仅仅能够表示线性函数了，他就能够表示任意的函数。

目前最常用的激活函数是ReLu函数。当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1。 当输入值等于0时，ReLU函数不可导。因此它不是一个纯粹的线性函数。

模型搭建好之后，我们还需要优化算法不断更新模型中每层神经元的参数，以期望模型能够更好的模拟整个现实问题。这个过程是通过前向传播和反向传播完成的。

## 2 卷积神经网络

图像是由像素组成的，对于彩色的像素，分为红绿蓝三个通道。因此图像是一个由高度、宽度和颜色组成的三维张量，比如一张分辨率为1024×1024的照片，使用三维张量表示就有1024×1024×3个元素。这么多的元素我们把它看作特征，如果使用基于线性回归的多层感知机，进行处理，参数会很多，需要大量的计算，并且不是图像中的所有像素都能够在图像分类任务中作为特征输入的，因此如果我们能够在图像中抽取出对于我们有用的特征，并且降低图像的分辨率（或者说降低传入后续神经网络需要处理的参数），把抽取后的特征图在输入全连接神经网络。这样的话对性能的要求就会小很多，这就需要我们寻找一种适合图像处理的网络结构。

而卷积神经网络的基础卷积层就是可以抽取图像的特征。完成我们提出的问题。

首先适合于计算机视觉的神经网络架构：

1. *平移不变性*（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。被检测对象在图像上的平移，仅导致被检测对象在隐藏层输出的特征图上的平移
2. *局部性*（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。

卷积神经网络是包含卷积层的一类特殊的神经网络。我们使用卷积核（convolution kernel）或者叫做滤波器（filter）称之为该卷积层的*权重*，通常该权重是可学习的参数。

通过卷积核的处理，我们抽取了图像的某一部分的特征，同时大幅减少了到下一层神经网络的输入，一次卷积只能获取到图像的某些特征，而且这些特征也不一定是我们想要的特征，因此我们不可能手动设计滤波器。我们应该通过机器学习由输入生成输出的卷积核

与之前线性模型中使用优化算法优化目标函数的权重和偏置类似，我们亦可以通过这种方法优化卷积核，先构造一个卷积层，然后初始化卷积核为随机张量，规定损失函数。在随后的前向传播和反向传播中迭代卷积核。学习到最接近我们想要的那一个卷积核。

# 残差网络

网络背景

在先前的卷积神经网络中，加深网络层数，对神经网络提升明显。但是如果在基础的卷积神经网络上继续叠加更深的层数，超过一定层数以后，却不一定能够使网络的效果变得更好，反而会让误差变大。

分析原因，我们得出在基础卷积神经网络上加深层数，训练过程中网络的正向信息和反向信息流动不通畅，网络并没有被充分训练。

残差网络的贡献

* 提出了一种残差模块，通过堆叠残差模块可以构建任意深度的神经网络，而不会出现退化现象
* 提出了批归一化方法来对抗梯度消失，该方法降低了网络训练过程中对于权重初始化的依赖

我们需要考虑这样一个问题，浅层网络学习到了有效的对图像进行分类方式后，如果我们网上堆积新层构建更深的网络，即使不提升浅层网络的性能，也不能够降低他的性能。
