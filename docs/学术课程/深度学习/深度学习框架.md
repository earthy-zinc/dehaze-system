# 深度学习框架

## Anaconda

### 虚拟环境的操作

#### 创建虚拟环境

* 指定环境名称：`conda  create  --name  env_name`
* 创建指定python版本：`conda  create  --name  env_name python=3.5` 
* 创建指定python版本下包含某些包：`conda  create  --name  env_name python=3.5 numpy scipy` 

#### 激活/使用/进入/退出某个虚拟环境

* `conda activate  env_name`
* `conda deactivate `

#### 复制某个虚拟环境
`conda  create  --name  new_env_name  --clone  old_env_name`

#### 删除某个环境
`conda  remove  --name  env_name  --all`

#### 查看当前所有环境
* `conda  info  --envs`   
* `conda  env  list`

#### 查看当前虚拟环境下的所有安装包
* `conda  list ` (需进入该虚拟环境)
* conda  list  -n  env_name

#### 安装或卸载包(进入虚拟环境之后）
* conda  install  xxx
* conda  install  xxx=版本号  # 指定版本号
* conda  install  xxx -i 源名称或链接 # 指定下载源
* conda  uninstall  xxx

#### 分享虚拟环境

conda env export > environment.yml  # 导出当前虚拟环境

conda env create -f environment.yml  # 创建保存的虚拟环境

#### 导出虚拟环境中所安装的包

* conda list -e > requirements.txt  # 导出
* conda install --yes --file requirements.txt  # 安装

### 虚拟环境镜像源

conda当前的源设置在$HOME/.condarc中，可通过文本查看器查看或者使用命令``conda config --show-sources`查看。

* conda config --show-sources #查看当前使用源
* conda config --remove channels 源名称或链接 #删除指定源
* conda config --add channels 源名称或链接 #添加指定源

#### 国内conda源

```shell
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge 
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/

```



### 升级和卸载

升级Anaconda需先升级conda

* `conda  update  conda`
* `conda  update  anaconda`
* `rm  -rf  anaconda`





## PIP

### pip导出环境

pip freeze > requirements.txt

pip install -r requirements.txt

### pip镜像源

`pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple/ `

#### 国内pip源

* 阿里云                    http://mirrors.aliyun.com/pypi/simple/
* 中国科技大学         https://pypi.mirrors.ustc.edu.cn/simple/ 
* 豆瓣(douban)         http://pypi.douban.com/simple/ 
* 清华大学                https://pypi.tuna.tsinghua.edu.cn/simple/
* 中国科学技术大学  http://pypi.mirrors.ustc.edu.cn/simple/

## NumPy

## Pandas

## OpenCV

## Pytorch

### 损失函数

神经网络中的损失函数同样可以划分为用于分类的损失函数和用于回归的损失函数。对于分类损失函数，一类是二分类的神经网络，其输出的激活函数是sigmoid函数，对应的损失函数是二分类交叉熵损失。由于激活函数是sigmoid，输出的是对应正分类的概率p，如果训练数据是正分类，那么l=1，否则为0.由于深度学习优化器一般是让损失函数最小，所以对应正分类和负分类的对数概率前面有个负号。于是当l为1时，因为对数函数时单调递增函数，优化损失函数使其最小也就意味着优化神经网络激活函数输出值p使其最大。对于多分类

### 优化器

### 池化层

深度学习的运算量与运算过程中的张量大小有关，大多数情况，输入图像大小是远大于输出大小。输入的张量太大，不仅不匹配最终的输出结果，还会使计算量变大。为了能够同时减小计算量，并且得到比较小的输出，神经网络会使用池化层来对中间的特征张量进行下采样，减小高度和宽度的大小。池化层没有任何参数张量和缓存张良，在深度学习过程中仅相当于改变维度大小的模块。

#### 最大池化层算法

原理是选定某一个卷积核的区域，取这个区域中输入张量的最大值，根据输入张量的形状不同，最大池化层分了三维最大池化

```python
class nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False):
    pass
```

第一个参数是kernerl_size，卷积核的大小。但是并不实际执行卷积运算，而是在输入张量的卷积核的区域内选择最大的值。可以看到这里使用了2 * 2的卷积核，步长为2，把整个4 * 4的区域分割成了每块2 * 2的四块区域，每块区域取当前区域的最大值。与卷积核的定义一样，一维的可以看作是一个整数或者是有一个元素的元组，二维的可以是一个整数和含有两个元素的元组。当取整数时，认为卷积核的宽和高都是这个数。三维的可以看作是一个整数和含有三个元素的元组。当我们取整数时，认为卷积核的宽高长都是这个数

第二个参数stride代表卷积核移动的步长，默认为None，意味着卷积核在某一维度移动的步长等于卷积核在该维度的大小，这时，两个相邻的卷积核没有重合的地方。

第三个参数padding指的是输入张量在边缘位置的填充，其意义和卷积层中padding参数的意义相同。

第四个参数dilation和扩张卷积定义相同，意味着参与最大池化的几个元素之间索引的差值。默认为1，意味着参与卷积的是空间上相邻的元素。

第五个参数return_indices决定是否返回最大元素所在的位置，默认为False，表示不反悔。

第六个参数是ceil_mode，意味着最大池化层最后的输出是否向上取整

#### 平均池化算法

计算的是所有元素的平均值

#### 乘幂平均池化算法

表示对卷积核区域选定的元素做对应的函数变换，可以看到平均池化和最大池化是乘幂平均池化的两个极端

#### 分数池化算法

默认情况下池化的输出是固定的，因为kernel_size和步长只能取一定的值，所以池化的输出也只能取离散的值。为了让池化的输出能够得到任意大小的值，我们可以采用分数池化算法。该算法的主要原理是给定卷积核的大小kernel_size在一定范围内随机选择步长的大小，使得最后的输出大小符合给定的输出 大小，或者使得输出大小和输入大小的比例符合输入的比例output_ratio，这个值在0-1之间，参数return_indices决定是否返回最大值所在的索引，和前文介绍过的同名参数作用一致。最后一个参数一般不需要指定

#### 自适应池化算法

通过输入张量的大小和输出形状的大小的值，根据一定的公式来计算需要的卷积核的大小。

#### 反池化算法

反池化是相反的操作，作为上采样的手段来替代转置卷积。由于池化会损失信息，二反池化需要补充信息，反池化中并没有可训练的参数来提供补充的信息，所以完整的等价于转置卷积的操作应该是反池化和卷积操作的组合，在这个组合中，反池化提供了上采样的操作，将输入张量扩张为给定的大小。卷积操作提供反池化中没有的信息。

### 归一化层

#### 批次归一化方法

```python
class BatchNorm2d(num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True):
    pass
```

这些类的构造函数需要传入num_features作为输入，这个 值对应的是输入张量的通道数目，对于四维张量，输入张量的形状为(N,C,H,W)即图像个数、通道数、长、宽。

第二个参数eps代表的是归一化公式中的一个小的浮点常数，这个常数的作用是为了防止计算过程中分母为零。

第三个参数momentum控制着指数移动平均计算平均值E(x)和方差Var(x)时的栋梁，

参数affine决定了是否在归一化之后做仿射变换

最后一个参数决定了是否使用指数移动平均来估计当前的统计参量，默认情况下是使用指数移动平均的方式。批次归一化不改变输入数据的形状，只改变输入张量的元素和数据分布

可以看到，无论是否使用指数移动平均对统计量进行估计，当前迷你批次的统计量的计算结果，包括它整体的平均值和方差都非常重要，不同归一化算法的重要区别之一就在于这些统计量的计算方法不同，对于批次归一化，其所有的统计量都是相对于批次计算的。对于输入是二维张量的情况下，统计量输入时相对于批次做平均，输入三维是相对于批次和宽度做平均。由于在求统计量的过程中包含了迷你批次N的平均所以称为批次归一化方法。

卷积层和批次归一化层结合使用，即卷积层的输出结果直接输入到批次归一化层中去，因为批次归一化层会减去卷积层输出结果的平均值，对于卷积层来说，偏置的参数会在减去平均的过程中被小区

在批次归一化的作用下，无论之前输入张量的元素分布是什么，最后都会被归一化成为平均值b和标准差y的分布，由于训练过程中迷你批次的数据仅仅时训练数据非常小的一部分，数据的变化会非常剧烈，这就导致了神经网络训练时的不稳定性，因为权重的梯度是和数据的值相关的，在标准化（归一化之后）相当与中间数据的分布都非常稳定，因此在一定程度上也稳定了权重梯度的变化，使之不至于变化过于激烈，从而增加了模型训练时的数值稳定性。某种意义上也加快了模型的收敛速度

在迷你批次比较小的情况下，批次归一化不是一个很好测选择，因为批次较小，可能会造成平均值和方差的波动较大，在这种情况下进行批次归一化，最后的结果反而会减小训练过程中的数值稳定性。为了解决这个问题，这里引入了组归一化的方法，减小了对批次的依赖性，而且不需要跨批次计算。

因为组归一化并不涉及迷你批次的维度，这里只需要指定通道分组的个数，通道的个数，通过指定这些组归一化将会对输入张量的通道分成多组，然后对每组分别进行归一化，这个模块的输入张量的形状为(N, C,*)模块并不会改变输入张量的形状。所有的通道被均匀分成了多份，分别对这几组张量按照(宽高分组大小)这个方向求平均数和方差的统计量。最后计算归一化和仿射变换。

#### 实例归一化

因为批次归一化会涉及当前迷你批次中所有输入张量，输入的某一张图片会引入其他图片的信息，在图像识别时，这种方法比较有用，因为在预测时就可以引入其他图片的信息做参照，在提高训练的数值稳定性的同时也提高了识别的健壮性。在另一些应用场景中引入其他图片信息可能对输出结果并没有什么帮助，甚至可能使输出效果下降。典型即生成图像的场景，如图像风格迁移，输入一幅图像，给定某种风格的图像，生成一幅和输入图像类似的有给定风格图图像。生成对抗网络，通过神经网络生成类似与输入图像的一系列随机图像。这种情况下生成一幅图像并不需要参考同一批次的其他图像。可以使用实例归一化

实例归一化相当于组归一化在组的数目等于通道数目的特殊情况

#### 层归一化

求的时除了迷你批次维度意外的所有其他维度的平均值和方差。层归一化对所有元素做放射变换。

pytorch中LayerNorm，第一个参数时normalized_shape需要传入输入张量除了迷你批次维度之外的其他维度的列表，对于思维张量来说，就需要传入通道数、宽高这个列表。默认情况下，使用的时每个元素的放射变换。层归一化算法适用于循环神经玩过，能够有效提高循环神经网络中隐含状态的稳定性。

如果把四维张量的四个通道展现在一个立方体上，其中立方体的三个方向分别时通道C、批次数N、高度和宽度HW。用蓝色立方体代表归一化的元素。批次归一化的方法是沿着通道，对批次数、高度和宽度进行标准（归一化）的。层归一化则是对所有的批次、高度、通道维度进行的；实例归一化是对宽度和高度维度进行的，组归一化则是对高度宽度以及部分的通道维度进行的。

### 丢弃层



### 生成模型

给定一组随机数，根据随机数来生成服从训练数据分布的数据。一个重要应用就是给定一组图像，构造出一个模型，在这组图像上进行训练，这个模型能够生成类似于给定训练图像中实例的图像。

### 变分自编码器

Variational AutoEncoder，VAE使用了两个神经网络。编码器，用于将输入图像转变为隐含变量Latent Variable，这个隐含变量的值需要加上一定的限制，使其服从目标分布。解码器的作用是输入隐含变量，将隐含变量转变为目标的语句。训练完成的模型只需要使用解码器的部分，通过输入随机产生的隐含变量的值，产生隐含变量对应的推向。

自编码器：在实际应用中，有时候需要根据输入的图像把图像映射成向量，从而对图像进行向量化表示，这是就需要使用自编码器对图像进行编码。自编码器对输入图像进行逐步降采样，输出降采样的中间编码表示，然后对中间编码表示进行升采样，输出最后的预测图像。最后的损失函数表示为输入图像和预测图像的相似度。

自编码器有两部分组成，第一部分是编码器，用于将输入图像做降采样，输出隐含变量；第二部分是解码器，用于将隐含变量做上采样，输出最后的预测图像。最后的损失函数由输入图像和输出图像共同决定。通过最小化损失函数，可以做到预测的输出图像和输入图像在结果上最接近，这样就能确保编码器能够正确地对图像进行编码，而解码器能够正确的对隐含变量进行解码，输出最接近输入函数的输出图像。这样就能确保隐含变量是输入图像的一个正确表示。在实际实现时，编码器可以使用卷积模块结合池化模块来实现。解码器可以使用卷积模块结合转置卷积来实现。

自编码器很好地解决了图像的编码问题，但是如果使用随机数产生隐含变量并不能输出正确的图像。因为输入图像编码的隐含变量可能没有一个正确的分布

### 生成对抗网络

Generative Adversarial Network，GAN对应的网络是生成网络Generator和判别网络Discriminator，生成网络的作用是输入服从一定分布的隐含变量，输出对应的图像。判别网络的作用是给定未知来源的图像，判断这个图像是属于神经网络生成的还是来源于训练数据集的，对应的损失函数也分为两个。第一个是生成网络生成的图像通过判别网络的损失函数，对于生成网络来说，对应的损失函数要使得判别网络难以判别出生成网络生成的图像是生成图像；对于判别网络来说，对应的损失函数要尽可能的判断出训练数据的图像，而且需要尽可能判断出生成的图像。生成网络和判别网络是一个对抗的过程，通过生成网络和判别网络的交替训练，最后达到模型的平衡状态，即判别网络无法判断出图像的真假，这样得到的生成网络就能够通过输入隐含变量输出对应的生成图像

对于VAE来说，优点是可以知道输入图像具体对应的隐含变量，但是缺点是生成的图像比较模糊。对于GAN来说，优点是生成的图像比较清晰，但是无法得到某一个输出图像对应的隐含变量，而且模型训练容易遇到不收敛和模式塌陷的问题。

## Matplotlib

## MMdetection

