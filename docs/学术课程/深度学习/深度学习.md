---
order: 1
---

# 深度学习基础

## 神经网络简介

受生物神经元的启发，人工神经元接收来自其他神经元或者外部源的输入，每个输入都有一个相关的权值，他是根据该输入对当前神经元的重要性来决定的，对该输入进行加权并于其他输入求和后，经过一个激活函数f，计算得到该神经元的输出。


那么我们利用神经元构建神经网络，相邻层之间的神经元相互连接，并且给每一个连接分配一个强度，也就是权值。而神经网络中的信息只能向一个方向流动，即从输入节点向前移动，通过中间好几层隐藏节点，再向输出节点移动，网络中没有环路。输入层就是输入x的那一层，输出层为输出y的那一层，隐藏层是输入层和输出层之间的那些层级。

总结来说神经网络的特点就是：

* 同一层神经元之间没有连接
* 第N层的每个神经元和第N+1层的所有神经元相连，第N-1层神经元的输出就是第N层神经元的输入
* 每个连接都有对应的权值


人工神经网络每个输入xi需要一个权重wi，同时所有的输入还需要一个偏置值b，然后对其求和后的结果输入到神经元的激活函数中，那么我们就出现了三个问题。

1. 在初始化的时候，我们如何设置每个输入的权重和所有输入的偏置值
2. 神经元的激活函数应该用哪一种
3. 整个神经元从输入到输出中间和实际情况之间的差距应该如何衡量，也就是如何构建损失函数

## 激活函数

在神经元中引入激活函数的目的就是向神经网络中引入非线性因素，通过激活函数，神经网络就可以拟合各种曲线，如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合，引入非线性函数作为激活函数，那输出就不再是输入的线性组合，可以逼近任意函数。

## 初始化参数

对于某一个神经元来说需要初始化的参数有两种：一类是权重，还有一类是偏置，偏置我们通常初始化为0，而权重的初始化比较重要，会影响神经网络的效率。一般参数的初始化有四种方法。

### 随机初始化

随机初始化从均值为1，标准差为1的高斯分布中取样，使用一些很小的值对权重w进行初始化

### 标准初始化

权重w会从一个区间中均匀的随机取值，在（-1/根号d，1/根号d）均匀分布中生成当前神经元的权重，其中d是每个神经元的输入数量。

### Xavier初始化

各层的激活值和梯度的方差在传播过程中保持一致也叫做glorot初始化

* 正态化的Xavier

以0为中心，标准差为stddev = sqrt(2/(输入神经元个数 +输出神经元个数))的正态分布中抽取样本。

### he初始化

在正向传播时，激活值的方差保持不变，反向传播时，关于状态值的梯度的方差保持不变。与Xavier类似，也有两种方法，正态化的和均匀化的。

## 搭建神经网络

### 通过sequential搭建

### 通过function类构建

将层作为可调用对象并返回张量，并将输入向量和输出向量提供给tf.keras.Model inputs outputs 参数

### 通过model的子类构建

需要在init函数中定义神经网络的层，同时在call方法中定义网络前向传播的过程

## 损失函数

### 分类任务中损失函数

分类任务中使用最多的衡量模型与实际之间差距的损失函数是交叉熵损失函数。

#### 多分类任务

这里通常使用softmax把原结果转换为概率的形式，所以也叫做softmax损失。这里的损失函数计算方法就是当前类别的标记值乘以log（softmax）函数的对数结果，对所有类别求和取负数就可。从概率的角度理解，我们的目的是最小化正确类别所对应的预测概率的对数的负值，也就是说正确类别所对应的预测概率值越大，那么损失就越小；其他错误类别所对应的预测概率值越小，损失相应的也就越小。损失函数计算出来的值越接近于0，说明模型预测产生的损失越小。

#### 二分类任务

在处理二分类任务的时候我们不使用softmax函数，而是用sigmoid函数，对应的损失函数也有所不同，意味着损失的计算方法略有不同，但是都是一样的，损失越小，损失函数计算出来的值就越小。

### 回归任务中损失函数

#### MAE损失

也叫做L1 LOSS 损失。用绝对误差作为距离。计算方法就是真实值与预测值之间差的绝对值，对于多个样本点，我们就要取所有误差的平均值。这种损失函数在函数零点是不可导的。在探索损失最小的运算中容易越过这个极小值。导致找不到最好的预测模型。

#### MSE损失

也叫做L2 LOSS 损失，或者欧式距离。与L1 loss之间的区别是，一个用绝对值，一个使用平方。也就是真实值与预测值之间的差的平方。但是这种情况下，我们从图像中就可以看到，函数离零点越远，他的导数值就越大，因此目标值和预测值相差很大的时候，会造成梯度爆炸。

#### smooth l1损失

结合了L1 L2损失的优缺点，L1损失在零点不可导，而L2损失在远处梯度过大。而smooth损失很好的解决了这两个问题，也就是结合了这两个损失函数，类似于在|x|<1的时候，用L2，其他情况下用L1，分段函数。

## 神经网络优化算法

### 梯度下降算法

梯度下降是一种寻找使损失函数最小化的方法，梯度的方向是函数增长速度最快的方向，梯度的反方向是函数减少最快的方向，沿着梯度下降，我们就能够找到损失函数的最小值。对于每一个输入权值，在深度学习中进行优化，有这样一个公式：新的权值=旧的权值-学习率 * 负梯度

其中学习率是我们应该考虑的一个重要因素，如果学习率太小，那么每次训练之后得到的效果都太小，增大训练的成本，如果学习率太大则很容易跳过最优解。一个很好的方法就是，学习率也要随着训练进行变化，一开始大，之后变小。

模型训练的概念：

| 词语      | 含义                                                         |
| --------- | ------------------------------------------------------------ |
| Epoch     | 使用训练集的全部数据对模型进行一次完整的训练                 |
| Batch     | 使用训练集的一小部分样本对模型的权重进行一次反向传播的参数更新 |
| Iteration | 使用一个batch数据对模型的参数进行一次更新的过程              |

### 反向传播算法

前向传播：指的是数据输入到神经网络中，逐层向前传播，一直运算到输出层为止。

反向传播：在网络训练过程中经过前向传播得到的最终结果和训练样本的真实值总是存在一定的误差，这个误差就是损失函数，要减小这个误差，从后往前依次求出各个参数的偏导，这就是反向传播。

链式法则：反向传播算法是利用链式法则进行梯度求解以及权重的更新的。对于复杂的复合函数，我们将其拆分为一系列的加减乘除或者指数，对数、三角函数等初等函数，通过链式法则（高数内容）完成复合函数的求导。

### 梯度下降优化方法

 在寻找损失函数最小值的时候，我们会遇到一些问题，比如函数在一段平坦区域内，下降的很慢，这一部分运用梯度下降算法会耗费许多运算能力，函数在某个区域中到达了鞍点也就是导数为0，但是不是极值点的情况，或者函数到达了局部极小值，但是却不是全局最小值。这些都会让深度神经网络模型无法达到最好的效果，那么人们就提出了一些解决方法来解决这个问题。主要有：动量算法、AdaGrad、RMSprop、Adam

#### 动量算法

主要解决鞍点的问题，首先是指数加权平均数，假定给一个序列，比如一年中每天气温值，一年中温度的波动会比较大，我们使用加权平均值进行平滑

#### AdaGrad

#### RMSprop

#### Adam



### 学习率退火

一般情况下，我们在训练神经网络的时候，会让学习率随着训练而变化，如果学习率过高，就找不到损失函数的最小点，如果学习率过低，就会造成收敛变慢的情况，会耗费大量的时间。我们调整学习率使用如下几种方法

#### 分段常数衰减

分段常数衰减是在事先定义好的训练次数区间上，设置不同的学习率常数，刚开始学习率大一些，之后越来越小，区间的设置需要根据样本量调整，一般样本量越大区间间隔越小。

在tf.keras 中对应的方法是：

```python
tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)
```

* boundaries: 设置分段更新的step值
* values针对不用分段的学习率值

比如说，我们设置对于前十万步，学习率为1.0，对于接下来的十万步到十一万，学习率设置为0.5，再然后的步骤将学习率设置为0.1以达到学习率逐步降低的结果。

#### 指数衰减

指数衰减就是学习率按照指数的图像逐步衰减，刚开始学习率为1，之后逐步下降，以一种指数的形式。这种情况下，刚开始衰减的慢，之后会越来越快。

在tf.keras 中对应的方法是：

```python
tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)
```

* initial_learning_rate: 初始的学习率
* decay_steps: k值
* decay_rate: 指数衰减公式中的底

#### 1/t衰减

也是用一种数学公式来表明随着步数的增加学习率减少的趋势，公式为a = a0/(1+kt)

总结来说，学习率退火就是让深度学习网络每次的学习率逐步下降的过程，防止学习率过大跳过最优点。

## 深度学习正则化

在设计机器学习算法时，不仅要求在训练集上的误差小，而且希望在新样本上的泛化能力强（也就是通用性更强）许多机器学习算法都采用相关的策略来减小测试误差，这些策略都称为正则化，因为神经网络经常会遇到过拟合现象，因此需要使用不同的策略来增强模型的泛化能力，也就是说需要使用不同形式的正则化策略。

正则化通过对算法的修改来减少泛化误差，目前使用较多的有参数范数惩罚，提前终止，DropOut等

### L1和L2正则化

正则化是减少泛化误差的方法，L1和L2正则化在损失函数上增加一个正则项，由于添加了这个正则化项，权重矩阵的值会减小，因为他们假定具有更小权重矩阵的神经网络会拥有更简单的网络模型。更简单的网络一定程度上会减少过拟合现象的发生。

* L2正则化

正则化后的损失函数 = 原损失函数 + a/2m * || w ||的平方

这里a属于正则化参数，是一个需要优化的超参数，L2正则化又叫做权重衰减，因为他会导致权重趋向于0

* L1正则化

L1正则化与L2正则化不同的是，权重w不采用绝对值的平方，这种情况下权重可能会被减少到0

在tf.keras中使用的方法是：

```
tf.keras.regularizers.L1(l1=0.01)
tf.keras.regularizers.L2(l2=0.01)
tf.keras.regularizers.L1L2(l1=0.0,l2=0.0)
```

### DropOut正则化

假设我们的神经网络是全连接的，在每个迭代过程中，随机选择某些节点，删除前向和后向连接，因此每个迭代过程都会有不同的节点组合，从而导致不同的输出，这可以看成机器学习中的集成方法，集成模型优于一般的单一模型，因为他们可以捕获更多的随机性，因此dropout模型使得神经网络模型优于正常的模型，这里我们把dropout理解为随机丢弃某些节点以及其对应的连接。

```
tf.keras.layers.Dropout(rate)
```

* rate: 表示每一个神经元被丢弃的概率

### 提前终止

提前停止也就是说将一部分训练集作为验证集，当验证集的性能越来越差或者性能不再提升的时候，立即停止对该模型的训练，也就是提前终止。

在实际应用中我们使用callbacks函数实现模型的早期终止

```
tf.keras.callbacks.EarlyStopping(
	monitor="val_loss", patience=5
)
```

* monitor: 表示监测量，val_loss表示验证集损失
* patience：表示epoch的数量

当整个训练过程中性能无提升的时候就会停止训练

### 批标准化

batch normalization 是对一批数据进行标准化的处理，是神经网络中的一层，简称BN层，与全连接层类似。但是批标准化层针对单个神经元进行，利用网络训练时的一个mini批次的数据来计算该神经元的均值和方差，归一化之后重构，因此叫做批标准化。

```
tf.keras.layers.BatchNormalization(
	epsilon=0.01, center=True, scale=True, beta_initializer="zeros", gamma_initializer="ones")
```

## 卷积神经网络

### 卷积神经网络的构成

卷积神经网络CNN收到人类视觉神经系统的启发，从原始信号摄入开始，接着做出初步的处理，大脑皮层某些细胞发现图形的边缘和方向，然后抽象物体，判定眼前物体的形状，是圆形的，然后进一步抽象判定该物体是一人脸。

而CNN网络主要有三部分构成：卷积层、池化层、全连接层，其中卷积层是负责提取图像中的局部特征，池化层用来降低参数的数量级，也就是降维操作，全连接层类似于人工神经网络部分，用来输出想要的结果。

### 卷积层

卷积层的目的是提取输入特征图的特征，可以提取图像中的边缘信息。卷积的本质就是在滤波器和输入数据的局部区域之间做点积。我们将图像看作是一个二维的矩阵，暂时忽略图像的RGB分量。而滤波器也是一个二维矩阵，只不过滤波器的特点是一般是3 * 3的矩阵，矩阵内由规律的01元素构成，通常滤波器的维度要比输入图像维度小很多，然后将输入图像的每一部分和滤波器（一个二维矩阵）做矩阵乘积，得到输出元素，进而得到输出矩阵，这个输出矩阵也就是特征图。

但是在卷积的过程中，得到的结果（特征图）相比原始图会小了一圈，我们可以在原图像周围扩张一个维度（也就是padding），扩张过后的图像再与卷积核（滤波器、二维矩阵）做点积，就能保证在卷积过程中特征图的大小保持不变。

因此我们有了三个关键点。一是卷积核应该如何构造，二是扩张出来的维度（padding）应该填充什么样的值呢。三是如何进行点积，也就是卷积核和原图像相乘步长应该是多少，通常来讲步长为1，也就是原图像从左上角开始，隔出来一个和卷积核大小相同的二维矩阵和卷积核相乘，得到的值放到结果矩阵的右上角，然后原图像中隔出来的小二维矩阵向右移动一个（步长为1的情况）继续和卷积核相乘，得到的值放在右上角的第二个位置，一次类推，得到最后结果。在步长增大的情况下。得到的特征图也会进一步缩小。 

实际情况下，当输入由多个通道的时候，卷积核也需要拥有相同的通道数，每个卷积核对应通道矩阵和输入层对应矩阵进行卷积，将每个通道卷积的结果按位相加可以的到最终的结果——叫做feature map特征图。

当然也会出现多个卷积核对原始图像进行卷积的情况，当有多个卷积核的时候，每个卷积核学习到不同的特征，对应产生包含多个通道的特征图。

针对输出特征图的大小，与以下参数息息相关，卷积核本身的大小，零填充原图像的方式，卷积核卷积的步长。

### 池化层

池化层主要是降低了后续网络层的输入维度，缩减模型的大小，提高计算速度，并且提高了特征图的健壮性，防止过拟合现象的发生，主要是对卷积层学习到的特征图进行下采样处理，这种下采样处理有两种，分别是最大池化、平均池化。

#### 最大池化

最大池化，也就是对特征矩阵建立窗口，窗口的尺寸要小于特征矩阵的维数，然后在每个窗口内部，取内部元素的最大值作为输出。

```
tf.keras.layer.MaxPool2D(
	pool_size=(2, 2), strides=None, padding="valid"
)
```

* pool_size: 池化窗口的大小
* strides 窗口移动的步长，默认1
* padding 是否进行填充，默认不填充

#### 平均池化

也就是取窗口内所有元素的均值作为输出

由以上的例子可以看出，在经过池化之后，输出的矩阵维度减少了，也就是意味着后续网络层输入的维度减小了，计算速度就会变快

### 全连接层

全连接层位于卷积神经网络的末端，经过卷积层的特征提取和池化层的降维之后，将特征图转化位一维向量送入到全连接层中进行分类或者回归操作。

### 卷积神经网络的构建

数据加载

数据处理

模型搭建

模型编译

模型训练

模型评估

## 图像分类

### 图像分类简介

### AlexNet

AlexNet网络也是在卷积神经网络框架下的一种模型，首先输入的是一个3通道矩阵，矩阵的维度位227 * 227，也就是说图像分辨率（或者说矩阵的维数是227）是227 * 227，在神经网络中经过卷积——池化——再卷积——池化——平整（转为1维）——全连接得到输出结果。

AlexNet包含8层变换，有5层卷积和2层全连接隐藏层，以及一个全连接输出层。

第一层卷积核的形状为11 * 11，卷积核的个数为96，卷积步长为4，对这个227 * 227 * 3的3通道图像（三个227 * 227的矩阵）做卷积，得到了55 * 55的特征图，一共有96个，其中每一个卷积核分别与三个通道中的每一个通道做卷积，得到的结果进行加和输出一个特征图。因此上述操作得到了形状为 55 * 55 * 96的三维特征图。

第二层为池化层，对特征矩阵建立3 * 3的窗口，步长设为2，采用最大池化的方法，特征图的维度减小到 27 * 27 * 96。达到初步降维的目的。

第三层还是卷积层，卷积核变小变为5 * 5的卷积核，个数为 256 。我们对上一步传下来的特征图进行范围填充（padding），填充过后使卷积后特征图的长和宽经过卷积之后不变。

第三层再次池化降维。和上次池化步骤相同。

第四层-第六层进行卷积，卷积核形状为3 * 3，同时也要进行范围填充保持特征图形状不变。

第七层进行池化，结果降维变成 6 * 6 * 256的三维特征图。

然后对这个三维特征图进行平整展开变为一维向量 6 * 6 * 256 = 9216 个参数。与原图像227 * 227 * 3 = 154587，相比参数降低了16倍。这些参数保留了图像的特征，同时降低了参数的个数，可以用于全连接层训练，减少计算量。这些参数同时作为神经元送入下层的全连接层

第八和第九层为全连接层，输入9216个参数到4096个神经元中，经过全连接的神经网络，进行学习。

第十层为全连接输出层，接受4096个参数（神经元），输出1000个分类结果。使用softmax将每个分类结果的得分转化为概率值，我们从中选取概率值最大的作为神经网络学习后的判断结果。

AlexNet网络将sigmoid激活函数改成了relu激活函数，计算更简单，网络更容易训练

通过dropout方法，也就是丢弃一些神经元来降低全连接层的复杂度

引入了大量图像增强方法，如反转，裁剪，颜色变化，从而进一步扩大数据集，缓解过拟合的现象。

### VGG

VGG神经网络可以看成是一个加深版的AlexNet，整个网络由卷积层和全连接层叠加形成，但是VGG网络在卷积层使用的卷积核都是小尺寸的卷积核。（3 * 3）池化层使用的池化窗口是2 * 2的，是通过不断加深卷积网络来提升性能（准确度、计算用时等）。

首先我们输入一个224 * 224 * 3 的三维矩阵（三通道RGB图片）经过两个 3 * 3 * 64 的卷积核（padding=same）生成特征图，然后2 * 2池化窗口池化，缩小维度，然后再卷积两次，池化，卷积三次，池化，卷积三次，池化。。。最后形成7 * 7 * 512 的特征矩阵，平整展开进入全连接层。

VGG模型中我们可以把整体模型分类好几个VGG块，组成规律是：连续使用多个相同的填充为1，卷积核大小为3 * 3的卷积层，后面接上一个步幅为2，窗口形状为2 * 2的最大池化层。卷积层保持输入的高和宽不变，而池化层则会对输入减半，我们定义一个vgg_block函数能够实现这个基础的VGG块，可以指定卷积层的数量num_convs和每层卷积核的个数num_filters

VGG模型

```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPool2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout

def vgg_block(num_convs, num_filters):
    block = Sequential()
    for _ in range(num_convs):
        block.add(
        	Conv2D(
                num_filters, kernel_size=3, padding="same",activation="relu"))
    block.add(MaxPool2D(pool_size=2, strides=2))
    return block

def vgg(conv_arch):
    net = Sequential()
    for (num_convs, num_filters) in conv_arch:
        net.add(vgg_block(num_convs, num_filters))
    net.add(Sequential([
        Flatten(),
        Dense(4096, activation="relu"),
        Dropout(0.5),
        Dense(4096, activation="relu"),
        Dropout(0.5),
        Dense(10, activation="softmax")
    ]))
    
def get_train(size):
    index = np.random.randint(0, np.shape(train_images)[0], size)
    resized_images = tf.image.resize_with_pad(train_images[index], 224, 224,)
    return resized_images.numpy(), train_labels[index]

def get_test(size):
    index = np.random.randint(0, np.shape(test_images)[0], size)
    resized_images = tf.image.resize_with_pad(test_images[index], 224, 224,)
    return resized_images.numpy(), test_labels[index]

conv_arch = ((2, 64), (2, 128), (3, 256), (3, 512), (3, 512))
net = vgg(conv_arch)

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = tf.reshape(train_images,(train_images.shape[0], train_images.shape[1], train_images.shape[2], 1))
test_images = tf.reshape(test_images,(test_images.shape[0], test_images.shape[1], test_images.shape[2], 1))

train_images = get_train(5000)
test_images = get_test(1000)

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False)
net.compile(optimizer=optimizer, loss="sparse_categorical_crossentropy", metrics=["accuracy"])

net.fit(train_images,train_labels, batch_size=256, epochs=5, verbose=1, validation_split=0.2)

net.evaluate(test_images, test_labels, verbose=1)
```

VGG16中这样的VGG块由5个，前两块使用两个卷积层，而后三块使用三个卷积层，第一块输出通道是64，之后每次输出通道数翻倍，直到变为512（矩阵的高度）

### GoogLeNet

googleNet在加深网络层数的同时做了结构上的创新，引入朗日叫做inception的结构代替之前的卷积加上激活的组件。

GoogleNet中的inception块里面有四条并行的线路，前三条线路使用窗口大小分别是 1 * 1， 3 * 3和5 * 5 的卷积层来抽取不同空间尺寸下的信息，其中中间2个香炉会对输入先做1 * 1的卷积来减少输入的通道数，用来降低模型的复杂度，第四条线路使用了3 * 3的最大池化层，后接1 * 1的卷积层来改变通道数，4条线路都使用了合适的填充来使得输入和输出的高度和宽度一致，最后我们将每条线路的输出在通道维上连接，并且向后进行传输，这个是GoogleNet中的一个网络块inception的结构。

1 * 1的卷积核：

1 * 1的大小做出的卷积，特点就是没有考虑特征图局部信息之间的关系，也就是说图像的像素和其周围像素之间的关系。那这个卷积核的作用是实现跨通道的交互和信息整合，实现卷积核通道数的降维、升维，减少网络参数



### ResNet



### 图像增强方法



### 模型微调

 

