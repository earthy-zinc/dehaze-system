---
order: 6
---

# ç¨‹åºä»£ç 
## Vueå‰ç«¯ä»£ç 
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <link rel="icon" href="/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="vue-element-adminçš„vue3ç‰ˆæœ¬" />
    <meta name="keywords" content="vue-element-admin,vue3-element-admin" />
    <title>å›¾åƒå»é›¾ç³»ç»Ÿ</title>
</head>
<body>
<div id="app" class="app"></div>
<script type="module" src="/src/main.ts"></script>
</body>
</html>
```
```js
import { createApp } from "vue";
import App from "./App.vue";
import router from "@/router";
import { setupStore } from "@/store";
import { setupDirective } from "@/directive";
import "@/permission";
// æœ¬åœ°SVGå›¾æ ‡
import "virtual:svg-icons-register";
// å›½é™…åŒ–
import i18n from "@/lang/index";
// æ ·å¼
import "element-plus/theme-chalk/dark/css-vars.css";
import "@/styles/index.scss";
import "uno.css";

const app = createApp(App);
// å…¨å±€æ³¨å†Œ è‡ªå®šä¹‰æŒ‡ä»¤(directive)
setupDirective(app);
// å…¨å±€æ³¨å†Œ çŠ¶æ€ç®¡ç†(store)
setupStore(app);
app.use(router).use(i18n).mount("#app");

import { DehazeIndex, ImageInfo, ModelInfo } from "@/api/dehaze/types";
import requestPy from "@/utils/request-py";
import { AxiosProgressEvent, AxiosPromise } from "axios";

export function getModelApi(): AxiosPromise<ModelInfo[]> {
    return requestPy({
        url: "/model/",
        method: "get",
    });
}

export function uploadImageApi(file: File): AxiosPromise<ImageInfo> {
    return requestPy({
        url: "/upload/",
        method: "post",
        data: file,
        headers: {
            "Content-Type": "Image/png",
        },
    });
}

export function downloadApi(image_name: string): AxiosPromise<File> {
    return requestPy({
        url: `/download/${image_name}/`,
        method: "get",
    });
}

export function dehazeApi(
    haze_image: string,
    model_name: string
): AxiosPromise<ImageInfo> {
    return requestPy({
        url: "/dehazeImage/",
        method: "post",
        data: { haze_image, model_name },
    });
}

export function calculateIndexApi(
    haze_image: string,
    clear_image: string,
    onUpload: ((progressEvent: AxiosProgressEvent) => void) | undefined
): AxiosPromise<DehazeIndex> {
    return requestPy({
        url: "/calculateIndex/",
        method: "post",
        data: { haze_image, clear_image },
        onUploadProgress: onUpload,
    });
}
export interface ModelInfo {
    value: string;
    label: string;
    children?: ModelInfo;
}

export interface DehazeIndex {
    psnr: string;
    ssim: string;
}

export interface ImageInfo {
    image_name: string;
}

// ç³»ç»Ÿè®¾ç½®
interface DefaultSettings {
    title: string;
    showSettings: boolean;
    tagsView: boolean;
    fixedHeader: boolean;
    sidebarLogo: boolean;
    layout: string;
    theme: string;
    size: string;
    language: string;
}

const defaultSettings: DefaultSettings = {
    title: "å›¾åƒå»é›¾ç³»ç»Ÿ",
    showSettings: true,
    tagsView: false,
    fixedHeader: false,
    sidebarLogo: true,
    layout: "left",
    theme: "light",
    size: "default", // default |large |small
    language: "zh-cn", // zh-cn| en
};

export default {
    // è·¯ç”±å›½é™…åŒ–
    route: {
        dashboard: "é¦–é¡µ",
        document: "é¡¹ç›®æ–‡æ¡£",
    },
    // ç™»å½•é¡µé¢å›½é™…åŒ–
    login: {
        title: "å›¾åƒå»é›¾ç³»ç»Ÿ",
        username: "ç”¨æˆ·å",
        password: "å¯†ç ",
        login: "ç™» å½•",
        verifyCode: "éªŒè¯ç ",
    },
    // å¯¼èˆªæ å›½é™…åŒ–
    navbar: {
        dashboard: "é¦–é¡µ",
        logout: "æ³¨é”€",
    },
};

// åˆ›å»º axios å®ä¾‹
import axios, { AxiosResponse } from "axios";

const service = axios.create({
    baseURL: import.meta.env.VITE_APP_PYTHON_API,
    timeout: 50000,
});

service.interceptors.response.use(
    (response: AxiosResponse) => {
        const { code, msg } = response.data;
        if (code === "00000") {
            return response.data;
        }
        if (response.data instanceof ArrayBuffer) {
            return response;
        }
        ElMessage.error(msg || response.data);
        return Promise.reject(new Error(msg || "Error"));
    },
    (error: any) => {
        if (error.response.data) {
            const { msg } = error.response.data;
            ElMessage.error(msg || "ç³»ç»Ÿå‡ºé”™");
        }
        return Promise.reject(error.message);
    }
);

export const imageBaseURL: string = import.meta.env.VITE_APP_IMG_URL;
// å¯¼å‡º axios å®ä¾‹
export default service;
export default defaultSettings;

import vue from "@vitejs/plugin-vue";
import { UserConfig, ConfigEnv, loadEnv, defineConfig } from "vite";
import AutoImport from "unplugin-auto-import/vite";
import Components from "unplugin-vue-components/vite";
import { ElementPlusResolver } from "unplugin-vue-components/resolvers";
import Icons from "unplugin-icons/vite";
import IconsResolver from "unplugin-icons/resolver";
import { createSvgIconsPlugin } from "vite-plugin-svg-icons";
import { viteMockServe } from "vite-plugin-mock";
import visualizer from "rollup-plugin-visualizer";
import UnoCSS from "unocss/vite";
import path from "path";
import viteCompression from "vite-plugin-compression";

const pathSrc = path.resolve(__dirname, "src");

export default defineConfig(({ mode }: ConfigEnv): UserConfig => {
    const env = loadEnv(mode, process.cwd());
    return {
        resolve: {
            alias: {
                "@": pathSrc,
            },
        },
        css: {
            // CSS é¢„å¤„ç†å™¨
            preprocessorOptions: {
                //define global scss variable
                scss: {
                    javascriptEnabled: true,
                    additionalData: `
            @use "@/styles/variables.scss" as *;
          `,
                },
            },
        },
        server: {
            host: "0.0.0.0",
            port: Number(env.VITE_APP_PORT),
            open: true, // è¿è¡Œæ˜¯å¦è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
            proxy: {
                // åå‘ä»£ç†è§£å†³è·¨åŸŸ
                [env.VITE_APP_BASE_API]: {
                    target: env.VITE_APP_TARGET_URL,
                    changeOrigin: true,
                    rewrite: (path) =>
                        path.replace(
                            new RegExp("^" + env.VITE_APP_BASE_API),
                            env.VITE_APP_TARGET_BASE_API
                        ), // æ›¿æ¢ /dev-api ä¸º target æ¥å£åœ°å€
                },
                [env.VITE_APP_PYTHON_API]: {
                    target: env.VITE_APP_PYTHON_URL,
                    changeOrigin: true,
                    rewrite: (path) =>
                        path.replace(
                            new RegExp("^" + env.VITE_APP_PYTHON_API),
                            env.VITE_APP_TARGET_BASE_API
                        ),
                },
            },
        },
        plugins: [
            vue(),
            UnoCSS({}),
            AutoImport({
                // è‡ªåŠ¨å¯¼å…¥ Vue ç›¸å…³å‡½æ•°ï¼Œå¦‚ï¼šref, reactive, toRef ç­‰
                imports: ["vue", "@vueuse/core"],
                eslintrc: {
                    enabled: false,
                    filepath: "./.eslintrc-auto-import.json",
                    globalsPropValue: true,
                },
                resolvers: [
                    // è‡ªåŠ¨å¯¼å…¥ Element Plus ç›¸å…³å‡½æ•°ï¼Œå¦‚ï¼šElMessage, ElMessageBox... (å¸¦æ ·å¼)
                    ElementPlusResolver(),
                    IconsResolver({}),
                ],
                vueTemplate: true,
                // é…ç½®æ–‡ä»¶ç”Ÿæˆä½ç½®(false:å…³é—­è‡ªåŠ¨ç”Ÿæˆ)
                // dts: false,
                dts: "src/types/auto-imports.d.ts",
            }),
            Components({
                resolvers: [
                    // è‡ªåŠ¨å¯¼å…¥ Element Plus ç»„ä»¶
                    ElementPlusResolver(),
                    // è‡ªåŠ¨å¯¼å…¥å›¾æ ‡ç»„ä»¶
                    IconsResolver({
                        // @iconify-json/ep æ˜¯ Element Plus çš„å›¾æ ‡åº“
                        enabledCollections: ["ep"],
                    }),
                ],
                // æŒ‡å®šè‡ªå®šä¹‰ç»„ä»¶ä½ç½®(é»˜è®¤:src/components)
                dirs: ["src/**/components"],
                // é…ç½®æ–‡ä»¶ä½ç½®(false:å…³é—­è‡ªåŠ¨ç”Ÿæˆ)
                // dts: false,
                dts: "src/types/components.d.ts",
            }),
            Icons({
                // è‡ªåŠ¨å®‰è£…å›¾æ ‡åº“
                autoInstall: true,
            }),
            createSvgIconsPlugin({
                // æŒ‡å®šéœ€è¦ç¼“å­˜çš„å›¾æ ‡æ–‡ä»¶å¤¹
                iconDirs: [path.resolve(pathSrc, "assets/icons")],
                // æŒ‡å®šsymbolIdæ ¼å¼
                symbolId: "icon-[dir]-[name]",
            }),
            // ä»£ç å‹ç¼©
            viteCompression({
                verbose: true, // é»˜è®¤å³å¯
                disable: true, // æ˜¯å¦ç¦ç”¨å‹ç¼©ï¼Œé»˜è®¤ç¦ç”¨ï¼Œtrueä¸ºç¦ç”¨,falseä¸ºå¼€å¯ï¼Œæ‰“å¼€å‹ç¼©éœ€é…ç½®nginxæ”¯æŒ
                deleteOriginFile: true, // åˆ é™¤æºæ–‡ä»¶
                threshold: 10240, // å‹ç¼©å‰æœ€å°æ–‡ä»¶å¤§å°
                algorithm: "gzip", // å‹ç¼©ç®—æ³•
                ext: ".gz", // æ–‡ä»¶ç±»å‹
            }),
            viteMockServe({
                ignore: /^_/,
                mockPath: "mock",
                enable: mode === "development",
                // https://github.com/anncwb/vite-plugin-mock/issues/9
            }),
            visualizer({
                filename: "./stats.html",
                open: false,
                gzipSize: true,
                brotliSize: true,
            }),
        ],
        // é¢„åŠ è½½é¡¹ç›®å¿…éœ€çš„ç»„ä»¶
        optimizeDeps: {
            include: [
                "vue",
                "vue-router",
                "pinia",
                "axios",
                "element-plus/es/components/form/style/css",
                "element-plus/es/components/form-item/style/css",
                "element-plus/es/components/button/style/css",
                "element-plus/es/components/input/style/css",
                "element-plus/es/components/input-number/style/css",
                "element-plus/es/components/switch/style/css",
                "element-plus/es/components/upload/style/css",
                "element-plus/es/components/menu/style/css",
                "element-plus/es/components/col/style/css",
                "element-plus/es/components/icon/style/css",
                "element-plus/es/components/row/style/css",
                "element-plus/es/components/tag/style/css",
                "element-plus/es/components/dialog/style/css",
                "element-plus/es/components/loading/style/css",
                "element-plus/es/components/radio/style/css",
                "element-plus/es/components/radio-group/style/css",
                "element-plus/es/components/popover/style/css",
                "element-plus/es/components/scrollbar/style/css",
                "element-plus/es/components/tooltip/style/css",
                "element-plus/es/components/dropdown/style/css",
                "element-plus/es/components/dropdown-menu/style/css",
                "element-plus/es/components/dropdown-item/style/css",
                "element-plus/es/components/sub-menu/style/css",
                "element-plus/es/components/menu-item/style/css",
                "element-plus/es/components/divider/style/css",
                "element-plus/es/components/card/style/css",
                "element-plus/es/components/link/style/css",
                "element-plus/es/components/breadcrumb/style/css",
                "element-plus/es/components/breadcrumb-item/style/css",
                "element-plus/es/components/table/style/css",
                "element-plus/es/components/tree-select/style/css",
                "element-plus/es/components/table-column/style/css",
                "element-plus/es/components/select/style/css",
                "element-plus/es/components/option/style/css",
                "element-plus/es/components/pagination/style/css",
                "element-plus/es/components/tree/style/css",
                "element-plus/es/components/alert/style/css",
                "element-plus/es/components/radio-button/style/css",
                "element-plus/es/components/checkbox-group/style/css",
                "element-plus/es/components/checkbox/style/css",
                "element-plus/es/components/tabs/style/css",
                "element-plus/es/components/tab-pane/style/css",
                "element-plus/es/components/rate/style/css",
                "element-plus/es/components/date-picker/style/css",
                "element-plus/es/components/notification/style/css",
                "@vueuse/core",
                "sortablejs",
                "path-to-regexp",
                "echarts",
                "@wangeditor/editor",
                "@wangeditor/editor-for-vue",
                "vue-i18n",
                "codemirror",
            ],
        },
    };
});
```
```vue
<script setup lang="ts">
  import { uploadImageApi } from "@/api/dehaze";
  import { imageBaseURL } from "@/utils/request-py";
  import { UploadRawFile, UploadRequestOptions } from "element-plus";

  const props = defineProps({
    modelValue: {
      type: String,
      default: "",
    },
    title: {
      type: String,
      default: "ç‚¹å‡»ä¸Šä¼ æ–‡ä»¶",
    },
  });
  const emit = defineEmits(["update:modelValue", "getImageInfo"]);
  const imageName = useVModel(props, "modelValue", emit);
  const imgUrl = computed(() => {
    return imageName.value ? imageBaseURL + "/" + imageName.value + "/" : "";
  });

  const isLoading = ref(false);
  async function uploadFile(options: UploadRequestOptions): Promise<any> {
    isLoading.value = true;
    try {
      const { data } = await uploadImageApi(options.file);
      imageName.value = data.image_name;
    } finally {
      isLoading.value = false;
    }
  }

  async function handleBeforeUpload(file: UploadRawFile) {
    if (file.size > 20 * 1024 * 1024) {
      ElMessage.warning("ä¸Šä¼ å›¾ç‰‡ä¸èƒ½å¤§äº20M");
      return false;
    }
    const image = new Image();
    image.src = URL.createObjectURL(file);
    await image.decode();
    // å‘çˆ¶ç»„ä»¶ä¼ é€’ç”¨æˆ·æ‰€ä¸Šä¼ çš„å›¾ç‰‡çš„åˆ†è¾¨ç‡
    emit("getImageInfo", image.naturalWidth, image.naturalHeight);
    return true;
  }
</script>

<template>
  <el-upload
      v-model="imgUrl"
      class="single-uploader"
      :show-file-list="false"
      list-type="picture"
      :before-upload="handleBeforeUpload"
      :http-request="uploadFile"
  >
    <img v-if="imgUrl" :src="imgUrl" class="single" alt="" />
    <div v-else-if="isLoading" class="single-uploader-placeholder">
      <span>ä¸Šä¼ ä¸­...</span>
    </div>
    <div v-else class="single-uploader-placeholder">
      <el-icon class="single-uploader-icon"><i-ep-plus /></el-icon>
      <span style="margin-top: 5px">{{ title }}</span>
    </div>
  </el-upload>
</template>

<style scoped>
  .single-uploader .single {
    display: block;
    width: 35vmax;
    height: 35vmax;
  }
</style>

<style>
  .single-uploader .el-upload {
    position: relative;
    overflow: hidden;
    cursor: pointer;
    border: 1px dashed var(--el-border-color);
    border-radius: 6px;
    transition: var(--el-transition-duration-fast);
  }

  .single-uploader .el-upload:hover {
    border-color: var(--el-color-primary);
  }

  .single-uploader-placeholder {
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    width: 35vmax;
    height: 35vmax;
  }

  .single-uploader-icon {
    font-size: 28px;
    color: #8c939d;
    text-align: center;
  }
</style>

<script setup lang="ts">
import { useUserStore } from "@/store/modules/user";
import { useTransition, TransitionPresets } from "@vueuse/core";

defineOptions({
  // eslint-disable-next-line
  name: "Dashboard",
  inheritAttrs: false,
});

const userStore = useUserStore();

const date: Date = new Date();

const greetings = computed(() => {
  const hours = date.getHours();
  if (hours >= 6 && hours < 8) {
    return "æ™¨èµ·æŠ«è¡£å‡ºè‰å ‚ï¼Œè½©çª—å·²è‡ªå–œå¾®å‡‰ğŸŒ…ï¼";
  } else if (hours >= 8 && hours < 12) {
    return "ä¸Šåˆå¥½ğŸŒï¼";
  } else if (hours >= 12 && hours < 18) {
    return "ä¸‹åˆå¥½â˜•ï¼";
  } else if (hours >= 18 && hours < 24) {
    return "æ™šä¸Šå¥½ğŸŒƒï¼";
  } else if (hours >= 0 && hours < 6) {
    return "å·å·å‘é“¶æ²³è¦äº†ä¸€æŠŠç¢æ˜Ÿï¼Œåªç­‰ä½ é—­ä¸Šçœ¼ç›æ’’å…¥ä½ çš„æ¢¦ä¸­ï¼Œæ™šå®‰ğŸŒ›ï¼";
  }
});

const duration = 5000;

// æ”¶å…¥é‡‘é¢
const amount = ref(0);
const amountOutput = useTransition(amount, {
  duration: duration,
  transition: TransitionPresets.easeOutExpo,
});
amount.value = 150;

// è®¿é—®æ•°
const visitCount = ref(0);
const visitCountOutput = useTransition(visitCount, {
  duration: duration,
  transition: TransitionPresets.easeOutExpo,
});
visitCount.value = 2180;

//æ¶ˆæ¯æ•°
const messageCount = ref(0);
const messageCountOutput = useTransition(messageCount, {
  duration: duration,
  transition: TransitionPresets.easeOutExpo,
});
messageCount.value = 15;

// è®¢å•æ•°
const orderCount = ref(0);
const orderCountOutput = useTransition(orderCount, {
  duration: duration,
  transition: TransitionPresets.easeOutExpo,
});
orderCount.value = 154;
</script>

<template>
  <div class="dashboard-container">
    <!-- ç”¨æˆ·ä¿¡æ¯ -->
    <el-row class="mb-8">
      <el-card class="w-full">
        <div class="flex justify-between flex-wrap">
          <div class="flex items-center">
            <img
              class="user-avatar"
              :src="userStore.avatar + '?imageView2/1/w/80/h/80'"
            />
            <span class="ml-[10px] text-[16px]">
              {{ userStore.nickname }}
            </span>
          </div>

          <div class="leading-[40px]">
            {{ greetings }}
          </div>

          <div class="space-x-2 flex items-center justify-end">
            <el-link target="_blank" type="danger" href="http://10.16.90.26/"
              >ğŸ’¥åœŸå‘³é”Œçš„é˜…è¯»ç¬”è®°</el-link
            >
            <el-divider direction="vertical" />
            <el-link
              target="_blank"
              type="success"
              href="https://gitee.com/earthy-zinc"
              >Gitee</el-link
            >
            <el-divider direction="vertical" />
            <el-link
              target="_blank"
              type="primary"
              href="https://github.com/earthy-zinc"
              >GitHub
            </el-link>
          </div>
        </div>
      </el-card>
    </el-row>

    <!-- æ•°æ®å¡ç‰‡ -->
    <el-row :gutter="40" class="mb-4">
      <el-col :xs="24" :sm="12" :lg="6" class="mb-4">
        <div class="data-box">
          <div
            class="text-[#40c9c6] hover:!text-white hover:bg-[#40c9c6] p-3 rounded"
          >
            <svg-icon icon-class="uv" size="3em" />
          </div>
          <div class="flex flex-col space-y-3">
            <div class="text-[var(--el-text-color-secondary)]">è®¿é—®æ•°</div>
            <div class="text-lg text-right">
              {{ Math.round(visitCountOutput) }}
            </div>
          </div>
        </div>
      </el-col>

      <!--æ¶ˆæ¯æ•°-->
      <el-col :xs="24" :sm="12" :lg="6" class="mb-4">
        <div class="data-box">
          <div
            class="text-[#36a3f7] hover:!text-white hover:bg-[#36a3f7] p-3 rounded"
          >
            <svg-icon icon-class="message" size="3em" />
          </div>
          <div class="flex flex-col space-y-3">
            <div class="text-[var(--el-text-color-secondary)]">æ¨¡å‹æ•°</div>
            <div class="text-lg text-right">
              {{ Math.round(messageCountOutput) }}
            </div>
          </div>
        </div>
      </el-col>

      <el-col :xs="24" :sm="12" :lg="6" class="mb-4">
        <div class="data-box">
          <div
            class="text-[#f4516c] hover:!text-white hover:bg-[#f4516c] p-3 rounded"
          >
            <svg-icon icon-class="money" size="3em" />
          </div>
          <div class="flex flex-col space-y-3">
            <div class="text-[var(--el-text-color-secondary)]">
              ç´¯è®¡å»é›¾æ•°é‡
            </div>
            <div class="text-lg text-right">
              {{ Math.round(amountOutput) }}
            </div>
          </div>
        </div>
      </el-col>
      <el-col :xs="24" :sm="12" :lg="6" class="mb-2">
        <div class="data-box">
          <div
            class="text-[#34bfa3] hover:!text-white hover:bg-[#34bfa3] p-3 rounded"
          >
            <svg-icon icon-class="shopping" size="3em" />
          </div>
          <div class="flex flex-col space-y-3">
            <div class="text-[var(--el-text-color-secondary)]">
              ç´¯è®¡è¯„ä¼°æ•°é‡
            </div>
            <div class="text-lg text-right">
              {{ Math.round(orderCountOutput) }}
            </div>
          </div>
        </div>
      </el-col>
    </el-row>

    <!-- Echarts å›¾è¡¨ -->
    <el-row :gutter="40">
      <el-col :sm="24" :lg="8" class="mb-4">
        <BarChart
          id="barChart"
          height="400px"
          width="100%"
          class="bg-[var(--el-bg-color-overlay)]"
        />
      </el-col>

      <el-col :xs="24" :sm="12" :lg="8" class="mb-4">
        <PieChart
          id="pieChart"
          height="400px"
          width="100%"
          class="bg-[var(--el-bg-color-overlay)]"
        />
      </el-col>

      <el-col :xs="24" :sm="12" :lg="8" class="mb-4">
        <RadarChart
          id="radarChart"
          height="400px"
          width="100%"
          class="bg-[var(--el-bg-color-overlay)]"
        />
      </el-col>
    </el-row>
  </div>
</template>

<style lang="scss" scoped>
.dashboard-container {
  position: relative;
  padding: 24px;

  .user-avatar {
    width: 40px;
    height: 40px;
    border-radius: 50%;
  }

  .data-box {
    display: flex;
    justify-content: space-between;
    padding: 20px;
    font-weight: bold;
    color: var(--el-text-color-regular);
    background: var(--el-bg-color-overlay);
    border-color: var(--el-border-color);
    box-shadow: var(--el-box-shadow-dark);
  }

  .svg-icon {
    fill: currentcolor !important;
  }
}
</style>
<!--  çº¿ + æŸ±æ··åˆå›¾ -->
<template>
  <el-card>
    <template #header>
      <div class="title">
        å»é›¾æ•ˆæœæŸ±çŠ¶å›¾
        <el-tooltip effect="dark" content="ç‚¹å‡»è¯•è¯•ä¸‹è½½" placement="bottom">
          <i-ep-download class="download" @click="downloadEchart" />
        </el-tooltip>
      </div>
    </template>

    <div :id="id" :class="className" :style="{ height, width }"></div>
  </el-card>
</template>

<script setup lang="ts">
  import * as echarts from "echarts";

  const props = defineProps({
    id: {
      type: String,
      default: "barChart",
    },
    className: {
      type: String,
      default: "",
    },
    width: {
      type: String,
      default: "200px",
      required: true,
    },
    height: {
      type: String,
      default: "200px",
      required: true,
    },
  });

  const options = {
    grid: {
      left: "2%",
      right: "2%",
      bottom: "10%",
      containLabel: true,
    },
    tooltip: {
      trigger: "axis",
      axisPointer: {
        type: "cross",
        crossStyle: {
          color: "#999",
        },
      },
    },
    legend: {
      x: "center",
      y: "bottom",
      data: ["æœ‰é›¾å›¾åƒ", "æ— é›¾å›¾åƒ", "PSNR", "SSIM"],
      textStyle: {
        color: "#999",
      },
    },
    xAxis: [
      {
        type: "category",
        data: [
          "C2PNet",
          "DehazeFormer",
          "MB-TaylorFormer",
          "MixDehazeNet",
          "RIDCP",
        ],
        axisPointer: {
          type: "shadow",
        },
      },
    ],
    yAxis: [
      {
        type: "value",
        min: 0,
        max: 3000,
        interval: 500,
        axisLabel: {
          formatter: "{value} ",
        },
      },
      {
        type: "value",
        min: 0,
        max: 50,
        interval: 10,
        axisLabel: {
          formatter: "{value}%",
        },
      },
    ],
    series: [
      {
        name: "æœ‰é›¾å›¾åƒ",
        type: "bar",
        data: [1200, 500, 2500, 1800, 800],
        barWidth: 20,
        itemStyle: {
          color: new echarts.graphic.LinearGradient(0, 0, 0, 1, [
            { offset: 0, color: "#83bff6" },
            { offset: 0.5, color: "#188df0" },
            { offset: 1, color: "#188df0" },
          ]),
        },
      },
      {
        name: "æ— é›¾å›¾åƒ",
        type: "bar",
        data: [3000, 1000, 2400, 1600, 800],
        barWidth: 20,
        itemStyle: {
          color: new echarts.graphic.LinearGradient(0, 0, 0, 1, [
            { offset: 0, color: "#25d73c" },
            { offset: 0.5, color: "#1bc23d" },
            { offset: 1, color: "#179e61" },
          ]),
        },
      },
      {
        name: "PSNR",
        type: "line",
        yAxisIndex: 1,
        data: [25, 30, 26, 38, 41],
        itemStyle: {
          color: "#67C23A",
        },
      },
      {
        name: "SSIM",
        type: "line",
        yAxisIndex: 1,
        data: [14, 25, 30, 35, 40],
        itemStyle: {
          color: "#409EFF",
        },
      },
    ],
  };
  const chart = ref<any>("");
  onMounted(() => {
    // å›¾è¡¨åˆå§‹åŒ–
    chart.value = markRaw(
        echarts.init(document.getElementById(props.id) as HTMLDivElement)
    );

    chart.value.setOption(options);

    // å¤§å°è‡ªé€‚åº”
    window.addEventListener("resize", () => {
      chart.value.resize();
    });
  });
  const downloadEchart = () => {
    // è·å–ç”»å¸ƒå›¾è¡¨åœ°å€ä¿¡æ¯
    const img = new Image();
    img.src = chart.value.getDataURL({
      type: "png",
      pixelRatio: 1,
      backgroundColor: "#fff",
    });
    // å½“å›¾ç‰‡åŠ è½½å®Œæˆåï¼Œç”Ÿæˆ URL å¹¶ä¸‹è½½
    img.onload = () => {
      const canvas = document.createElement("canvas");
      canvas.width = img.width;
      canvas.height = img.height;
      const ctx = canvas.getContext("2d");
      if (ctx) {
        ctx.drawImage(img, 0, 0, img.width, img.height);
        const link = document.createElement("a");
        link.download = `å»é›¾æ•ˆæœå›¾.png`;
        link.href = canvas.toDataURL("image/png", 0.9);
        document.body.appendChild(link);
        link.click();
        link.remove();
      }
    };
  };
</script>
<style lang="scss" scoped>
  .title {
    display: flex;
    justify-content: space-between;

    .download {
      cursor: pointer;

      &:hover {
        color: #409eff;
      }
    }
  }
</style>
<!-- æ¼æ–—å›¾ -->
<template>
  <div :id="id" :class="className" :style="{ height, width }"></div>
</template>

<script setup lang="ts">
  import * as echarts from "echarts";

  const props = defineProps({
    id: {
      type: String,
      default: "funnelChart",
    },
    className: {
      type: String,
      default: "",
    },
    width: {
      type: String,
      default: "200px",
      required: true,
    },
    height: {
      type: String,
      default: "200px",
      required: true,
    },
  });

  const options = {
    title: {
      show: true,
      text: "è®¢å•çº¿ç´¢è½¬åŒ–æ¼æ–—å›¾",
      x: "center",
      padding: 15,
      textStyle: {
        fontSize: 18,
        fontStyle: "normal",
        fontWeight: "bold",
        color: "#337ecc",
      },
    },
    grid: {
      left: "2%",
      right: "2%",
      bottom: "10%",
      containLabel: true,
    },
    legend: {
      x: "center",
      y: "bottom",
      data: ["Show", "Click", "Visit", "Inquiry", "Order"],
    },

    series: [
      {
        name: "Funnel",
        type: "funnel",
        left: "20%",
        top: 60,
        bottom: 60,
        width: "60%",
        sort: "descending",
        gap: 2,
        label: {
          show: true,
          position: "inside",
        },
        labelLine: {
          length: 10,
          lineStyle: {
            width: 1,
            type: "solid",
          },
        },
        itemStyle: {
          borderColor: "#fff",
          borderWidth: 1,
        },
        emphasis: {
          label: {
            fontSize: 20,
          },
        },
        data: [
          { value: 60, name: "Visit" },
          { value: 40, name: "Inquiry" },
          { value: 20, name: "Order" },
          { value: 80, name: "Click" },
          { value: 100, name: "Show" },
        ],
      },
    ],
  };

  onMounted(() => {
    const chart = echarts.init(
        document.getElementById(props.id) as HTMLDivElement
    );
    chart.setOption(options);

    window.addEventListener("resize", () => {
      chart.resize();
    });
  });
</script>
<!-- é¥¼å›¾ -->
<template>
  <el-card>
    <template #header> å»é›¾æ–¹æ³•é¥¼å›¾ </template>
    <div :id="id" :class="className" :style="{ height, width }"></div>
  </el-card>
</template>

<script setup lang="ts">
  import * as echarts from "echarts";

  const props = defineProps({
    id: {
      type: String,
      default: "pieChart",
    },
    className: {
      type: String,
      default: "",
    },
    width: {
      type: String,
      default: "200px",
      required: true,
    },
    height: {
      type: String,
      default: "200px",
      required: true,
    },
  });
  const options = {
    grid: {
      left: "2%",
      right: "2%",
      bottom: "10%",
      containLabel: true,
    },
    legend: {
      top: "bottom",
      textStyle: {
        color: "#999",
      },
    },
    series: [
      {
        name: "Nightingale Chart",
        type: "pie",
        radius: [50, 130],
        center: ["50%", "50%"],
        roseType: "area",
        itemStyle: {
          borderRadius: 1,
          color: function (params: any) {
            //è‡ªå®šä¹‰é¢œè‰²
            const colorList = ["#409EFF", "#67C23A", "#E6A23C", "#F56C6C"];
            return colorList[params.dataIndex];
          },
        },
        data: [
          { value: 58, name: "è§£ç å™¨-ç¼–ç å™¨" },
          { value: 27, name: "Transformer" },
          { value: 10, name: "æ— ç›‘ç£" },
          { value: 5, name: "ç‰©ç†æ¨¡å‹" },
        ],
      },
    ],
  };

  onMounted(() => {
    const chart = echarts.init(
        document.getElementById(props.id) as HTMLDivElement
    );
    chart.setOption(options);
    window.addEventListener("resize", () => {
      chart.resize();
    });
  });
</script>
<!-- é›·è¾¾å›¾ -->
<template>
  <el-card>
    <template #header> æ•°æ®é›†æƒ…å†µé›·è¾¾å›¾ </template>
    <div :id="id" :class="className" :style="{ height, width }"></div>
  </el-card>
</template>

<script setup lang="ts">
  import * as echarts from "echarts";

  const props = defineProps({
    id: {
      type: String,
      default: "radarChart",
    },
    className: {
      type: String,
      default: "",
    },
    width: {
      type: String,
      default: "200px",
      required: true,
    },
    height: {
      type: String,
      default: "200px",
      required: true,
    },
  });

  const options = {
    grid: {
      left: "2%",
      right: "2%",
      bottom: "10%",
      containLabel: true,
    },
    legend: {
      x: "center",
      y: "bottom",
      data: ["ç®€å•é›¾éœ¾å›¾", "å›°éš¾é›¾éœ¾å›¾", "çœŸå®é›¾éœ¾å›¾"],
      textStyle: {
        color: "#999",
      },
    },
    radar: {
      // shape: 'circle',
      radius: "60%",
      indicator: [
        { name: "RESIDE" },
        { name: "Dense-Haze" },
        { name: "I-Haze" },
        { name: "O-Haze" },
        { name: "RS-Haze" },
        { name: "NH-Haze" },
      ],
    },
    series: [
      {
        name: "Budget vs spending",
        type: "radar",
        itemStyle: {
          borderRadius: 6,
          color: function (params: any) {
            //è‡ªå®šä¹‰é¢œè‰²
            const colorList = ["#409EFF", "#67C23A", "#E6A23C", "#F56C6C"];
            return colorList[params.dataIndex];
          },
        },
        data: [
          {
            value: [400, 100, 200, 600, 300, 100],
            name: "ç®€å•é›¾éœ¾å›¾",
          },
          {
            value: [300, 100, 100, 200, 600, 100],
            name: "å›°éš¾é›¾éœ¾å›¾",
          },
          {
            value: [800, 300, 200, 100, 600, 500],
            name: "çœŸå®é›¾éœ¾å›¾",
          },
        ],
      },
    ],
  };

  onMounted(() => {
    const chart = echarts.init(
        document.getElementById(props.id) as HTMLDivElement
    );
    chart.setOption(options);

    window.addEventListener("resize", () => {
      chart.resize();
    });
  });
</script>
<script setup lang="ts">
  import { calculateIndexApi, dehazeApi, getModelApi } from "@/api/dehaze";
  import { ModelInfo } from "@/api/dehaze/types";
  import SingleUploadPy from "@/components/Upload/SingleUploadPy.vue";
  import { useAppStore } from "@/store/modules/app";
  import { imageBaseURL } from "@/utils/request-py";
  import { ElMessage } from "element-plus";

  const dehazeModels = ref<ModelInfo[]>();
  const selectedDehazeModel = ref("");
  const appStore = useAppStore();

  onMounted(async () => {
    const { data } = await getModelApi();
    dehazeModels.value = data;
  });

  const hazeImage = reactive({
    name: "",
    height: 0,
    width: 0,
  });
  const clearImage = reactive({
    name: "",
    height: 0,
    width: 0,
  });
  const outputImage = reactive({
    name: "",
    height: 0,
    width: 0,
  });
  const dehazedImgUrl = computed(() => {
    return outputImage.name ? imageBaseURL + "/" + outputImage.name + "/" : "";
  });

  const isLoading = ref(false);
  const loadingText = ref("å¼€å§‹å»é›¾");

  async function dehazeImage() {
    if (selectedDehazeModel.value.length === 0) {
      ElMessage.warning("è¯·é€‰æ‹©å»é›¾æ¨¡å‹ï¼");
      return;
    }
    if (hazeImage.name.length === 0) {
      ElMessage.warning("è¯·ä¸Šä¼ æœ‰é›¾å›¾åƒï¼");
      return;
    }
    isLoading.value = true;
    loadingText.value = "æ­£åœ¨å»é›¾";
    try {
      const { data } = await dehazeApi(hazeImage.name, selectedDehazeModel.value);
      outputImage.name = data.image_name;
    } finally {
      isLoading.value = false;
      loadingText.value = "å¼€å§‹å»é›¾";
    }
  }

  const psnr = ref(0);
  const psnrEvaluate = computed(() => {
    if (psnr.value > 40) return "å¥‡è¿¹";
    else if (psnr.value > 30) return "ä¼˜ç§€";
    else if (psnr.value > 25) return "è‰¯å¥½";
    else if (psnr.value > 20) return "ä¸€èˆ¬";
    else if (psnr.value > 1) return "ä¸åŠæ ¼";
    else return "-";
  });
  const ssim = ref(0);
  const ssimEvaluate = computed(() => {
    if (ssim.value > 0.95) return "å¥‡è¿¹";
    else if (ssim.value > 0.8) return "ä¼˜ç§€";
    else if (ssim.value > 0.6) return "è‰¯å¥½";
    else if (ssim.value > 0.4) return "ä¸€èˆ¬";
    else if (ssim.value > 0.01) return "ä¸åŠæ ¼";
    else return "-";
  });
  const vi = ref(0);
  const viEvaluate = computed(() => "-");
  const ri = ref(0);
  const riEvaluate = computed(() => "-");
  const comprehensiveReview = computed(() => {
    if (ssim.value === 0 || psnr.value === 0) return "-";
    const score = (psnr.value / 40 + ssim.value / 1) / 2;
    let result;
    if (score > 0.95) result = "ç®€ç›´æ˜¯å¥‡è¿¹";
    else if (score > 0.8) result = "éå¸¸ä¼˜ç§€";
    else if (score > 0.6) result = "è‰¯å¥½";
    else if (score > 0.4) result = "ä¸€èˆ¬";
    else if (score > 0) result = "å¤ªå·®äº†ï¼ä¸åŠæ ¼";
    else result = "æ— æ³•è¯„ä»·";
    return `è¯¥å›¾åƒå»é›¾æ•ˆæœ${result}ï¼Œåœ¨PSNRè¡¨ç°${psnrEvaluate.value}ã€SSIMæŒ‡æ ‡ä¸Šçš„è¡¨ç°${ssimEvaluate.value}ï¼`;
  });

  async function calculateDehazeIndex() {
    if (
        hazeImage.height !== clearImage.height ||
        hazeImage.width !== clearImage.width
    ) {
      ElMessage.warning("åŸºå‡†æ— é›¾å›¾åƒå’Œä¼ å…¥çš„æœ‰é›¾å›¾åƒåˆ†è¾¨ç‡ä¸å¯¹åº”ï¼Œæ— æ³•è®¡ç®—æŒ‡æ ‡");
      return;
    }
    if (outputImage.name.length === 0 || clearImage.name.length === 0) {
      ElMessage.warning("æœªç‚¹å‡»å»é›¾æˆ–è€…æœªä¸Šä¼ åŸºå‡†æ— é›¾å›¾åƒï¼Œæ— æ³•å»é›¾");
      return;
    }
    const { data } = await calculateIndexApi(
        outputImage.name,
        clearImage.name,
        (progressEvent) => {
          if (progressEvent.total) {
            evaluatePercentage.value = Math.round(
                (progressEvent.loaded / progressEvent.total) * 100
            );
          } else {
            evaluatePercentage.value = 100;
          }
        }
    );
    psnr.value = Math.floor(parseFloat(data.psnr) * 100) / 100;
    ssim.value = Math.floor(parseFloat(data.ssim) * 10000) / 10000;
  }

  function clearEvaluateResult() {
    clearImage.name = "";
    clearImage.height = 0;
    clearImage.width = 0;
    evaluatePercentage.value = 0;
    psnr.value = 0;
    ssim.value = 0;
  }

  function getHazeImageInfo(width: number, height: number) {
    hazeImage.width = width;
    hazeImage.height = height;
  }

  function getClearImageInfo(width: number, height: number) {
    clearImage.width = width;
    clearImage.height = height;
  }

  function resetForm() {
    clearEvaluateResult();
    outputImage.name = "";
    outputImage.height = 0;
    outputImage.width = 0;
    hazeImage.name = "";
    hazeImage.width = 0;
    hazeImage.height = 0;
    selectedDehazeModel.value = "";
  }

  const dialogVisible = ref(false);
  const drawerVisible = ref(false);
  const evaluatePercentage = ref(0);
</script>

<template>
  <div class="app-container">
    <h1 style="margin: 5px 0 10px; text-align: center">å¤šæ¨¡å‹å›¾åƒå»é›¾ç³»ç»Ÿ</h1>

    <div class="operate-panel">
      <el-cascader
          placeholder="è¯·é€‰æ‹©å»é›¾æ¨¡å‹"
          v-model="selectedDehazeModel"
          :options="dehazeModels"
          filterable
          :props="{ emitPath: false }"
          :clearable="true"
      />

      <div class="operate-panel-right">
        <el-button type="primary" @click="dehazeImage" :loading="isLoading"
        >{{ loadingText }}
        </el-button>
        <el-button @click="dialogVisible = !dialogVisible">è¯„ä¼°æ•ˆæœ</el-button>
        <el-button @click="drawerVisible = !drawerVisible">å†å²è®°å½•</el-button>
        <el-button type="warning" @click="resetForm">é‡ç½®é¡µé¢</el-button>
      </div>
    </div>

    <div class="image-show-container">
      <single-upload-py
          v-model="hazeImage.name"
          title="ä¸Šä¼ æœ‰é›¾å›¾åƒ"
          @get-image-info="getHazeImageInfo"
      />
      <span style="margin: 0 20px"></span>
      <el-image
          style="width: 35vmax; height: 35vmax"
          :src="dehazedImgUrl"
          fit="fill"
          alt="@/assets/photo.png"
      >
        <template #error>
          <div class="image-show-placeholder">
            <div class="image-show-text">
              è¯·ä¸Šä¼ æœ‰é›¾å›¾åƒ<br />å¹¶ç‚¹å‡»"å¼€å§‹å»é›¾"è·å–æ— é›¾å›¾åƒ
            </div>
          </div>
        </template>
      </el-image>
    </div>

    <el-dialog
        v-model="dialogVisible"
        :width="appStore.device === 'mobile' ? '100%' : '70%'"
        title="è¯„ä¼°æ•ˆæœ"
        top="2vh"
        class="dialog-class"
    >
      <el-alert
          :closable="false"
          show-icon
          description="è¯„ä¼°å»é›¾æ¨¡å‹çš„å»é›¾æ•ˆæœéœ€è¦ä¸Šä¼ ç”¨äºæ¯”è¾ƒçš„åŸºå‡†æ— é›¾å›¾åƒï¼Œç³»ç»Ÿæ‰èƒ½å¤Ÿè®¡ç®—å‡ºæ¨¡å‹å»é›¾æ•ˆæœå’ŒçœŸå®çš„æ— é›¾å›¾åƒä¹‹é—´çš„å·®è·ã€‚çœŸå®çš„æ— é›¾å›¾åƒéœ€è¦å’ŒåŸå§‹çš„æœ‰é›¾å›¾åƒæ‹æ‘„ä½ç½®ã€å›¾åƒå®½é«˜å¤§å°ä¸€è‡´ï¼Œå¦åˆ™ä¼šå‡ºç°é”™è¯¯"
      />
      <div class="dialog-content">
        <single-upload-py
            style="margin-right: 20px"
            v-model="clearImage.name"
            title="ä¸Šä¼ åŸºå‡†æ— é›¾å›¾åƒ"
            @get-image-info="getClearImageInfo"
        />
        <div class="dialog-content-right">
          <div class="dialog-content-right-up">
            <el-button type="primary" @click="calculateDehazeIndex"
            >å¼€å§‹è¯„ä¼°
            </el-button>
            <el-button type="info" @click="clearEvaluateResult"
            >æ¸…ç©ºç»“æœ
            </el-button>
          </div>
          <div class="dialog-content-right-center">
            <div style="display: flex; justify-content: center; width: 100%">
              <el-progress
                  v-if="evaluatePercentage !== 100"
                  type="dashboard"
                  :percentage="evaluatePercentage"
                  :color="0"
              />
            </div>
            <el-result v-if="evaluatePercentage === 100" icon="success" />
          </div>
          <div>
            <div class="text-class">
              <span style="margin-right: 16px">é›¾éœ¾å›¾åƒåˆ†è¾¨ç‡</span>
              {{ hazeImage.width + " * " + hazeImage.height }}
            </div>
            <div class="text-class">
              <span style="margin-right: 16px">åŸºå‡†å›¾åƒåˆ†è¾¨ç‡</span>
              {{ clearImage.width + " * " + clearImage.height }}
            </div>
            <el-descriptions :column="2" size="large">
              <el-descriptions-item :span="1" :min-width="50" label="PSNR"
              >{{ psnr }}
              </el-descriptions-item>
              <el-descriptions-item :min-width="50" label="è¯„ä»·"
              >{{ psnrEvaluate }}
              </el-descriptions-item>
              <el-descriptions-item :min-width="50" label="SSIM"
              >{{ ssim }}
              </el-descriptions-item>
              <el-descriptions-item :min-width="50" label="è¯„ä»·"
              >{{ ssimEvaluate }}
              </el-descriptions-item>
              <el-descriptions-item :min-width="50" label="VI"
              >{{ vi }}
              </el-descriptions-item>
              <el-descriptions-item :min-width="50" label="è¯„ä»·"
              >{{ viEvaluate }}
              </el-descriptions-item>
              <el-descriptions-item :min-width="50" label="RI"
              >{{ ri }}
              </el-descriptions-item>
              <el-descriptions-item :min-width="50" label="è¯„ä»·"
              >{{ riEvaluate }}
              </el-descriptions-item>
              <el-descriptions-item :width="150" label="ç»¼åˆç‚¹è¯„">
                {{ comprehensiveReview }}
              </el-descriptions-item>
            </el-descriptions>
          </div>
        </div>
      </div>
    </el-dialog>

    <el-drawer
        :size="appStore.device === 'mobile' ? '100%' : '650'"
        v-model="drawerVisible"
        direction="rtl"
        title="å†å²è®°å½•"
    >
      <el-table>
        <el-table-column prop="id" label="åºå·" />
        <el-table-column prop="hazeModel" label="å»é›¾æ¨¡å‹" />
        <el-table-column prop="type" label="æ“ä½œç±»åˆ«" />
        <el-table-column prop="operationTime" label="æ“ä½œæ—¶é—´" />
        <el-table-column prop="detail" label="è¯¦æƒ…" />
      </el-table>
    </el-drawer>
  </div>
</template>

<style>
  .operate-panel {
    display: flex;
    flex-wrap: wrap;
    justify-content: space-between;
    margin: 0 15px 20px;
  }

  .operate-panel-right {
    display: flex;
    flex-wrap: wrap;
    align-content: space-between;
  }

  .image-show-container {
    display: flex;
    flex-wrap: wrap;
    justify-content: center;
    margin: 0 15px;
  }

  .image-show-placeholder {
    display: flex;
    align-content: center;
    align-items: center;
    justify-content: center;
    width: 35vmax;
    height: 35vmax;
    border: 1px dashed var(--el-border-color);
    border-radius: 6px;
  }

  .image-show-text {
    text-align: center;
  }

  .dialog-content {
    display: flex;
    flex-wrap: wrap;
    justify-content: space-between;
    margin-top: 15px;
  }

  .dialog-content-right {
    display: flex;
    flex-direction: column;
    flex-grow: 1;
    justify-content: space-between;
  }

  .dialog-content-right-up {
    display: flex;
    justify-content: space-evenly;
  }

  .dialog-content-right-center {
    height: 130px;
  }

  .text-class {
    padding: 1px 1px 16px;
    color: var(--el-text-color-primary);
  }
</style>
<template>
  <div class="login-container">
    <el-form
        ref="loginFormRef"
        :model="loginData"
        :rules="loginRules"
        class="login-form"
    >
      <div class="flex text-white items-center py-4 title-wrap">
        <span class="text-2xl flex-1 text-center title">
          {{ $t("login.title") }}
        </span>
        <lang-select class="text-white! cursor-pointer" />
      </div>

      <el-form-item prop="username">
        <div class="p-2 text-white">
          <svg-icon icon-class="user" />
        </div>
        <el-input
            ref="username"
            v-model="loginData.username"
            class="flex-1"
            size="large"
            :placeholder="$t('login.username')"
            name="username"
        />
      </el-form-item>

      <el-tooltip
          :disabled="isCapslock === false"
          content="Caps lock is On"
          placement="right"
      >
        <el-form-item prop="password">
          <span class="p-2 text-white">
            <svg-icon icon-class="password" />
          </span>
          <el-input
              v-model="loginData.password"
              class="flex-1"
              placeholder="å¯†ç "
              :type="passwordVisible === false ? 'password' : 'input'"
              size="large"
              name="password"
              @keyup="checkCapslock"
              @keyup.enter="handleLogin"
          />
          <span class="mr-2" @click="passwordVisible = !passwordVisible">
            <svg-icon
                :icon-class="passwordVisible === false ? 'eye' : 'eye-open'"
                class="text-white cursor-pointer"
            />
          </span>
        </el-form-item>
      </el-tooltip>

      <!-- éªŒè¯ç  -->
      <el-form-item prop="verifyCode">
        <span class="p-2 text-white">
          <svg-icon icon-class="verify_code" />
        </span>
        <el-input
            v-model="loginData.verifyCode"
            auto-complete="off"
            :placeholder="$t('login.verifyCode')"
            class="w-[60%]"
            @keyup.enter="handleLogin"
        />

        <div class="captcha">
          <img :src="captchaBase64" @click="getCaptcha" />
        </div>
      </el-form-item>

      <el-button
          size="default"
          :loading="loading"
          type="primary"
          class="w-full"
          @click.prevent="handleLogin"
      >{{ $t("login.login") }}
      </el-button>

      <!-- è´¦å·å¯†ç æç¤º -->
      <div class="mt-4 text-white text-sm">
        <span>å»é›¾ä½“éªŒè´¦å·: dehaze</span>
        <span class="ml-4"> å¯†ç : 123456</span>
      </div>
    </el-form>
  </div>
</template>

<script setup lang="ts">
  import router from "@/router";
  import LangSelect from "@/components/LangSelect/index.vue";
  import SvgIcon from "@/components/SvgIcon/index.vue";

  // çŠ¶æ€ç®¡ç†ä¾èµ–
  import { useUserStore } from "@/store/modules/user";

  // APIä¾èµ–
  import { LocationQuery, LocationQueryValue, useRoute } from "vue-router";
  import { getCaptchaApi } from "@/api/auth";
  import { LoginData } from "@/api/auth/types";

  const userStore = useUserStore();
  const route = useRoute();

  /**
   * æŒ‰é’®loading
   */
  const loading = ref(false);
  /**
   * æ˜¯å¦å¤§å†™é”å®š
   */
  const isCapslock = ref(false);
  /**
   * å¯†ç æ˜¯å¦å¯è§
   */
  const passwordVisible = ref(false);
  /**
   * éªŒè¯ç å›¾ç‰‡Base64å­—ç¬¦ä¸²
   */
  const captchaBase64 = ref();

  /**
   * ç™»å½•è¡¨å•å¼•ç”¨
   */
  const loginFormRef = ref(ElForm);

  const loginData = ref<LoginData>({
    username: "",
    password: "",
  });

  const loginRules = {
    username: [{ required: true, trigger: "blur" }],
    password: [{ required: true, trigger: "blur", validator: passwordValidator }],
    verifyCode: [{ required: true, trigger: "blur" }],
  };

  /**
   * å¯†ç æ ¡éªŒå™¨
   */
  function passwordValidator(rule: any, value: any, callback: any) {
    if (value.length < 6) {
      callback(new Error("The password can not be less than 6 digits"));
    } else {
      callback();
    }
  }

  /**
   * æ£€æŸ¥è¾“å…¥å¤§å°å†™çŠ¶æ€
   */
  function checkCapslock(e: any) {
    const { key } = e;
    isCapslock.value = key && key.length === 1 && key >= "A" && key <= "Z";
  }

  /**
   * è·å–éªŒè¯ç 
   */
  function getCaptcha() {
    getCaptchaApi().then(({ data }) => {
      const { verifyCodeBase64, verifyCodeKey } = data;
      loginData.value.verifyCodeKey = verifyCodeKey;
      captchaBase64.value = verifyCodeBase64;
    });
  }

  /**
   * ç™»å½•
   */
  function handleLogin() {
    loginFormRef.value.validate((valid: boolean) => {
      if (valid) {
        loading.value = true;
        userStore
            .login(loginData.value)
            .then(() => {
              const query: LocationQuery = route.query;

              const redirect = (query.redirect as LocationQueryValue) ?? "/";

              const otherQueryParams = Object.keys(query).reduce(
                  (acc: any, cur: string) => {
                    if (cur !== "redirect") {
                      acc[cur] = query[cur];
                    }
                    return acc;
                  },
                  {}
              );

              router.push({ path: redirect, query: otherQueryParams });
            })
            .catch(() => {
              // éªŒè¯å¤±è´¥ï¼Œé‡æ–°ç”ŸæˆéªŒè¯ç 
              getCaptcha();
            })
            .finally(() => {
              loading.value = false;
            });
      }
    });
  }

  onMounted(() => {
    getCaptcha();
  });
</script>

<style lang="scss" scoped>
  .login-container {
    width: 100%;
    min-height: 100%;
    overflow: hidden;
    background-color: #2d3a4b;

    .title-wrap {
      filter: contrast(30);

      .title {
        letter-spacing: 4px;
        animation: showup 3s forwards;
      }

      @keyframes showup {
        0% {
          letter-spacing: -20px;
        }

        100% {
          letter-spacing: 4px;
        }
      }
    }

    .login-form {
      width: 520px;
      max-width: 100%;
      padding: 160px 35px 0;
      margin: 0 auto;
      overflow: hidden;

      .captcha {
        position: absolute;
        top: 0;
        right: 0;

        img {
          width: 120px;
          height: 48px;
          cursor: pointer;
        }
      }
    }
  }

  .el-form-item {
    background: rgb(0 0 0 / 10%);
    border: 1px solid rgb(255 255 255 / 10%);
    border-radius: 5px;
  }

  .el-input {
    background: transparent;

    // å­ç»„ä»¶ scoped æ— æ•ˆï¼Œä½¿ç”¨ :deep
    :deep(.el-input__wrapper) {
      padding: 0;
      background: transparent;
      box-shadow: none;

      .el-input__inner {
        color: #fff;
        background: transparent;
        border: 0;
        border-radius: 0;
        caret-color: #fff;

        &:-webkit-autofill {
          box-shadow: 0 0 0 1000px transparent inset !important;
          -webkit-text-fill-color: #fff !important;
        }

        // è®¾ç½®è¾“å…¥æ¡†è‡ªåŠ¨å¡«å……çš„å»¶è¿Ÿå±æ€§
        &:-webkit-autofill,
        &:-webkit-autofill:hover,
        &:-webkit-autofill:focus,
        &:-webkit-autofill:active {
          transition: color 99999s ease-out, background-color 99999s ease-out;
          transition-delay: 99999s;
        }
      }
    }
  }
</style>

```
## JAVAåç«¯ä»£ç 
```java
package com.pei.dehaze;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class SystemApplication {
    public static void main(String[] args) {
        SpringApplication.run(SystemApplication.class, args);
    }
}
package com.youlai.system.controller;

import cn.hutool.core.util.StrUtil;
import com.pei.dehaze.common.constant.SecurityConstants;
import com.pei.dehaze.common.result.Result;
import com.pei.dehaze.common.util.RequestUtils;
import com.pei.dehaze.security.captcha.EasyCaptchaService;
import com.pei.dehaze.model.dto.CaptchaResult;
import com.pei.dehaze.model.dto.LoginResult;
import com.pei.dehaze.security.JwtTokenManager;
import io.jsonwebtoken.Claims;
import io.swagger.v3.oas.annotations.Parameter;
import io.swagger.v3.oas.annotations.security.SecurityRequirement;
import io.swagger.v3.oas.annotations.tags.Tag;
import io.swagger.v3.oas.annotations.Operation;
import jakarta.servlet.http.HttpServletRequest;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.security.authentication.AuthenticationManager;
import org.springframework.security.authentication.UsernamePasswordAuthenticationToken;
import org.springframework.security.core.Authentication;
import org.springframework.security.core.context.SecurityContextHolder;
import org.springframework.web.bind.annotation.*;

import java.util.Date;
import java.util.concurrent.TimeUnit;

@Tag(name = "è®¤è¯ä¸­å¿ƒ")
@RestController
@RequestMapping("/api/v1/auth")
@RequiredArgsConstructor
@Slf4j
public class AuthController {
    private final AuthenticationManager authenticationManager;
    private final JwtTokenManager jwtTokenManager;
    private final EasyCaptchaService easyCaptchaService;
    private final RedisTemplate redisTemplate;

    @Operation(summary = "ç™»å½•")
    @PostMapping("/login")
    public Result<LoginResult> login(
            @Parameter(description = "ç”¨æˆ·å", example = "admin") @RequestParam String username,
            @Parameter(description = "å¯†ç ", example = "123456") @RequestParam String password
    ) {
        UsernamePasswordAuthenticationToken authenticationToken = new UsernamePasswordAuthenticationToken(
                username.toLowerCase().trim(),
                password
        );
        Authentication authentication = authenticationManager.authenticate(authenticationToken);
        // ç”Ÿæˆtoken
        String accessToken = jwtTokenManager.createToken(authentication);
        LoginResult loginResult = LoginResult.builder()
                .tokenType("Bearer")
                .accessToken(accessToken)
                .build();
        return Result.success(loginResult);
    }

    @Operation(summary = "æ³¨é”€", security = {@SecurityRequirement(name = SecurityConstants.TOKEN_KEY)})
    @DeleteMapping("/logout")
    public Result logout(HttpServletRequest request) {
        String token = RequestUtils.resolveToken(request);
        if (StrUtil.isNotBlank(token)) {
            Claims claims = jwtTokenManager.getTokenClaims(token);
            String jti = claims.get("jti", String.class);

            Date expiration = claims.getExpiration();
            if (expiration != null) {
                // æœ‰è¿‡æœŸæ—¶é—´ï¼Œåœ¨tokenæœ‰æ•ˆæ—¶é—´å†…å­˜å…¥é»‘åå•ï¼Œè¶…å‡ºæ—¶é—´ç§»é™¤é»‘åå•èŠ‚çœå†…å­˜å ç”¨
                long ttl = (expiration.getTime() - System.currentTimeMillis());
                redisTemplate.opsForValue().set(SecurityConstants.BLACK_TOKEN_CACHE_PREFIX + jti, null, ttl, TimeUnit.MILLISECONDS);
            } else {
                // æ— è¿‡æœŸæ—¶é—´ï¼Œæ°¸ä¹…åŠ å…¥é»‘åå•
                redisTemplate.opsForValue().set(SecurityConstants.BLACK_TOKEN_CACHE_PREFIX + jti, null);
            }
        }
        SecurityContextHolder.clearContext();
        return Result.success("æ³¨é”€æˆåŠŸ");
    }

    @Operation(summary = "è·å–éªŒè¯ç ")
    @GetMapping("/captcha")
    public Result getCaptcha() {
        CaptchaResult captcha = easyCaptchaService.getCaptcha();
        return Result.success(captcha);
    }
}
```
## pythonåç«¯ä»£ç 
```python
#!/usr/bin/env python
"""Django's command-line utility for administrative tasks."""
import os
import sys
def main():
    """Run administrative tasks."""
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'dehazing_system.settings')
    try:
        from django.core.management import execute_from_command_line
    except ImportError as exc:
        raise ImportError(
            "Couldn't import Django. Are you sure it's installed and "
            "available on your PYTHONPATH environment variable? Did you "
            "forget to activate a virtual environment?"
        ) from exc
    execute_from_command_line(sys.argv)

if __name__ == '__main__':
    main()

from pathlib import Path
BASE_DIR = Path(__file__).resolve().parent.parent

DEBUG = True

ALLOWED_HOSTS = ['*']

# APPEND_SLASH = False
# æœ€å¤§æ–‡ä»¶ä¸Šä¼ å¤§å° 20MBï¼ˆå•ä½ï¼šå­—èŠ‚ï¼‰
DATA_UPLOAD_MAX_MEMORY_SIZE = 20971520

INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
]

MIDDLEWARE = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
]

ROOT_URLCONF = 'dehazing_system.urls'

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [BASE_DIR / 'templates']
        ,
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
            ],
        },
    },
]

WSGI_APPLICATION = 'dehazing_system.wsgi.application'

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': BASE_DIR / 'db.sqlite3',
    }
}

AUTH_PASSWORD_VALIDATORS = [
    {
        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
    },
]
LANGUAGE_CODE = 'en-US'
TIME_ZONE = 'UTC'
USE_I18N = True
USE_TZ = True

STATIC_URL = 'static/'

DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'

import os
import uuid
import torch
DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'
PROJECT_PATH = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(PROJECT_PATH, "data")
MODEL_PATH = os.path.join(PROJECT_PATH, "trained_model")
if __name__ == '__main__':
    image_name = str(uuid.uuid4()) + ".png"
    image_path = os.path.join(DATA_PATH, image_name)
    print(image_path)

import json
import os.path
import traceback
import uuid
from django.http import HttpResponse, HttpRequest
import benchmark.C2PNet.run
import benchmark.DehazeFormer.run
import benchmark.MixDehazeNet.run
import benchmark.CMFNet.run
import benchmark.DEANet.run
import benchmark.FogRemoval.run
import benchmark.ITBdehaze.run
import benchmark.RIDCP.run
from benchmark.metrics import calculate
from global_variable import DATA_PATH

dehaze_model = {
    'C2PNet/OTS.pkl': benchmark.C2PNet.run.dehaze,
    'C2PNet/ITS.pkl': benchmark.C2PNet.run.dehaze,
    'DehazeFormer/indoor/dehazeformer-b.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/indoor/dehazeformer-d.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/indoor/dehazeformer-l.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/indoor/dehazeformer-m.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/indoor/dehazeformer-s.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/indoor/dehazeformer-t.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/indoor/dehazeformer-w.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/outdoor/dehazeformer-b.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/outdoor/dehazeformer-m.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/outdoor/dehazeformer-s.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/outdoor/dehazeformer-t.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/reside6k/dehazeformer-b.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/reside6k/dehazeformer-m.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/reside6k/dehazeformer-s.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/reside6k/dehazeformer-t.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/rshaze/dehazeformer-b.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/rshaze/dehazeformer-m.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/rshaze/dehazeformer-s.pth': benchmark.DehazeFormer.run.dehaze,
    'DehazeFormer/rshaze/dehazeformer-t.pth': benchmark.DehazeFormer.run.dehaze,
    'MixDehazeNet/haze4k/MixDehazeNet-l.pth': benchmark.MixDehazeNet.run.dehaze,
    'MixDehazeNet/indoor/MixDehazeNet-l.pth': benchmark.MixDehazeNet.run.dehaze,
    'MixDehazeNet/indoor/MixDehazeNet-b.pth': benchmark.MixDehazeNet.run.dehaze,
    'MixDehazeNet/outdoor/MixDehazeNet-b.pth': benchmark.MixDehazeNet.run.dehaze,
    'MixDehazeNet/outdoor/MixDehazeNet-l.pth': benchmark.MixDehazeNet.run.dehaze,
    'MixDehazeNet/outdoor/MixDehazeNet-s.pth': benchmark.MixDehazeNet.run.dehaze,
    'CMFNet/dehaze_I_OHaze_CMFNet.pth': benchmark.CMFNet.run.dehaze,
    'DEA-Net/HAZE4K/PSNR3426_SSIM9885.pth': benchmark.DEANet.run.dehaze,
    'DEA-Net/ITS/PSNR4131_SSIM9945.pth': benchmark.DEANet.run.dehaze,
    'DEA-Net/OTS/PSNR3659_SSIM9897.pth': benchmark.DEANet.run.dehaze,
    'FogRemoval/NH-HAZE_params_0100000.pt': benchmark.FogRemoval.run.dehaze,
    'ITBdehaze/best.pkl': benchmark.ITBdehaze.run.dehaze,
    'RIDCP/pretrained_RIDCP.pth': benchmark.RIDCP.run.dehaze,
}


def ok_response(data):
    message = {
        'code': '00000',
        'msg': 'ä¸€åˆ‡ok',
        'data': data
    }
    return HttpResponse(json.dumps(message), content_type='application/json')


def error_response(code, msg):
    message = {
        'code': code,
        'msg': msg,
        'data': None
    }
    return HttpResponse(json.dumps(message), content_type='application/json')


def get_model(request: HttpRequest):
    result = []
    for index, key in enumerate(dehaze_model):
        # é¦–å…ˆå°†å­—ç¬¦ä¸²æŒ‰ç…§ / åˆ†å‰²æˆæ•°ç»„
        parts = key.split('/')
        # ç„¶åè·å–å½“å‰å·²ç»ç»„è£…å¥½çš„ç»“æœï¼Œå‡†å¤‡ç»§ç»­å‘å†…éƒ¨æ·»åŠ å½“å‰ç»“ç‚¹
        current = result
        # éå†è¯¥æ•°ç»„ï¼Œåˆ›å»ºåµŒå¥—çš„æ•°ç»„
        for i, part in enumerate(parts):
            # å¦‚æœå½“å‰å…ƒç´ æ˜¯æ•°ç»„çš„æœ€åä¸€ä¸ªå…ƒç´ ï¼Œä¹Ÿå°±æ˜¯'DehazeFormer/indoor/dehazeformer-b.pth' ä¸­çš„ 'dehazeformer-b.pth'
            # é‚£ä¹ˆå°±å°†å½“å‰å…ƒç´ æ”¾å…¥ç»“æœæ•°ç»„ä¸­
            if i == len(parts) - 1:
                current.append({'value': key, 'label': part.split(".")[0]})
            else:
                # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªå…ƒç´ ï¼Œåˆ™éå†ç»“æœæ•°ç»„ï¼Œç›´åˆ°æ‰¾åˆ°ä¸€ä¸ªkeyå’Œå½“å‰çš„å…ƒç´ ä¸€æ ·çš„
                # å°±æ›´æ”¹å½“å‰ç»“æœæ•°ç»„
                found = False
                for child in current:
                    if child['value'] == part:
                        current = child['children']
                        found = True
                        break
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™åˆ›å»ºä¸€ä¸ªæ–°å…ƒç´ ï¼Œæ’å…¥åˆ°ç»“æœæ•°ç»„ä¸­ï¼Œå¹¶ä¸”æ›´æ–°å½“å‰ç»“æœæ•°ç»„
                if not found:
                    new_node = {'value': part, 'label': part, 'children': []}
                    current.append(new_node)
                    current = new_node['children']
    return ok_response(result)


def upload_image(request: HttpRequest):
    image_name = str(uuid.uuid4()) + ".png"
    image_path = os.path.join(DATA_PATH, image_name)
    image = request.body
    # ä¿å­˜å‰ç«¯ä¼ æ¥çš„å›¾ç‰‡
    with open(image_path, "wb") as destination:
        destination.write(image)
    return ok_response({'image_name': image_name})


def download_image(request: HttpRequest, image_name: str):
    image_path = os.path.join(DATA_PATH, image_name)
    with open(image_path, "rb") as destination:
        return HttpResponse(destination.read(), content_type="image/png")


def dehaze_image(request: HttpRequest):
    data = json.loads(request.body)
    haze_image_name = data["haze_image"]
    model_name = data["model_name"]

    output_image_name = str(uuid.uuid4()) + ".png"
    haze_image_path = os.path.join(DATA_PATH, haze_image_name)
    output_image_path = os.path.join(DATA_PATH, output_image_name)

    try:
        dehaze = dehaze_model.get(model_name, None)
        if dehaze is not None:
            dehaze(haze_image_path, output_image_path, model_name)
        else:
            return error_response('1', "æ— æ³•æ‰¾åˆ°æ¨¡å‹")
    except RuntimeError as e:
        traceback.print_exc()
        return error_response('1', e.__str__())

    return ok_response({'image_name': output_image_name})


def calculate_dehaze_index(request: HttpRequest):
    data = json.loads(request.body)
    haze_image_name = data["haze_image"]
    clear_image_name = data["clear_image"]
    haze_image_path = os.path.join(DATA_PATH, haze_image_name)
    clear_image_path = os.path.join(DATA_PATH, clear_image_name)

    psnr, ssim = calculate(haze_image_path, clear_image_path)
    return ok_response({'psnr': psnr, 'ssim': ssim})

from django.contrib import admin
from django.urls import path
import dehazing_system.photo
urlpatterns = [
    path('admin/', admin.site.urls),
    path('model/', dehazing_system.photo.get_model),
    path("upload/", dehazing_system.photo.upload_image),
    path('download/<str:image_name>/', dehazing_system.photo.download_image),
    path('dehazeImage/', dehazing_system.photo.dehaze_image),
    path('calculateIndex/', dehazing_system.photo.calculate_dehaze_index),
]
import numpy as np
from PIL import Image
from skimage.metrics import peak_signal_noise_ratio, structural_similarity


def calculate(haze_image_path: str, clear_image_path: str):
    haze = Image.open(haze_image_path).convert('RGB')
    clear = Image.open(clear_image_path).convert('RGB')
    haze = np.array(haze)
    clear = np.array(clear)
    current_psnr = peak_signal_noise_ratio(haze, clear)
    current_ssim = structural_similarity(haze, clear, channel_axis=2)
    return current_psnr, current_ssim

import torch
from PIL import Image
import torchvision.transforms as tfs
import torchvision.utils as torch_utils
from skimage.metrics import peak_signal_noise_ratio, structural_similarity
from benchmark.C2PNet.model import C2PNet
import os
# from benchmark.C2PNet.metrics import psnr, ssim
from global_variable import MODEL_PATH, DEVICE

def get_model(model_name: str):
    # æ„é€ æ¨¡å‹æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    model_dir = os.path.join(MODEL_PATH, model_name)
    net = C2PNet(gps=3, blocks=19)
    ckp = torch.load(model_dir)
    net = net.to(DEVICE)
    net.load_state_dict(ckp['model'])
    net.eval()
    return net

def dehaze(haze_image_path: str, output_image_path: str, model_name: str = 'C2PNet/OTS.pkl'):
    net = get_model(model_name)
    haze = Image.open(haze_image_path).convert('RGB')
    haze = tfs.ToTensor()(haze)[None, ::]
    haze = haze.to(DEVICE)
    with torch.no_grad():
        pred = net(haze)
    ts = torch.squeeze(pred.clamp(0, 1).cpu())
    torch_utils.save_image(ts, output_image_path)

import torch
import torch.nn as nn


def default_conv(in_channels, out_channels, kernel_size, bias=True):
    return nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size // 2), bias=bias)


class CALayer(nn.Module):
    def __init__(self, channel):
        super(CALayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.ca = nn.Sequential(
            nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),
            nn.ReLU(inplace=True),
            nn.Conv2d(channel // 8, channel, 1, padding=0, bias=True),
            nn.Sigmoid()
        )

    def forward(self, x):
        y = self.avg_pool(x)
        y = self.ca(y)
        return x * y


class PDU(nn.Module):  # physical block
    def __init__(self, channel):
        super(PDU, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.ka = nn.Sequential(
            nn.Conv2d(channel, channel // 8, 1, padding=0, bias=True),
            nn.ReLU(inplace=True),
            nn.Conv2d(channel // 8, channel, 1, padding=0, bias=True),
            nn.Sigmoid()
        )
        self.td = nn.Sequential(
            default_conv(channel, channel, 3),
            default_conv(channel, channel // 8, 3),
            nn.ReLU(inplace=True),
            default_conv(channel // 8, channel, 3),
            nn.Sigmoid()
        )

    def forward(self, x):
        a = self.avg_pool(x)
        a = self.ka(a)
        t = self.td(x)
        j = torch.mul((1 - t), a) + torch.mul(t, x)
        return j


class Block(nn.Module):  # origin
    def __init__(self, conv, dim, kernel_size, ):
        super(Block, self).__init__()
        self.conv1 = conv(dim, dim, kernel_size, bias=True)
        self.act1 = nn.ReLU(inplace=True)
        self.conv2 = conv(dim, dim, kernel_size, bias=True)
        self.calayer = CALayer(dim)
        self.pdu = PDU(dim)

    def forward(self, x):
        res = self.act1(self.conv1(x))
        res = res + x
        res = self.conv2(res)
        res = self.calayer(res)
        res = self.pdu(res)
        res += x
        return res


class Group(nn.Module):
    def __init__(self, conv, dim, kernel_size, blocks):
        super(Group, self).__init__()
        modules = [Block(conv, dim, kernel_size) for _ in range(blocks)]
        modules.append(conv(dim, dim, kernel_size))
        self.gp = nn.Sequential(*modules)

    def forward(self, x):
        res = self.gp(x)
        res += x
        return res


class C2PNet(nn.Module):
    def __init__(self, gps, blocks, conv=default_conv):
        super(C2PNet, self).__init__()
        self.gps = gps
        self.dim = 64
        kernel_size = 3
        pre_process = [conv(3, self.dim, kernel_size)]
        assert self.gps == 3
        self.g1 = Group(conv, self.dim, kernel_size, blocks=blocks)
        self.g2 = Group(conv, self.dim, kernel_size, blocks=blocks)
        self.g3 = Group(conv, self.dim, kernel_size, blocks=blocks)
        self.ca = nn.Sequential(*[
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(self.dim * self.gps, self.dim // 16, 1, padding=0),
            nn.ReLU(inplace=True),
            nn.Conv2d(self.dim // 16, self.dim * self.gps, 1, padding=0, bias=True),
            nn.Sigmoid()
        ])
        self.pdu = PDU(self.dim)

        post_precess = [
            conv(self.dim, self.dim, kernel_size),
            conv(self.dim, 3, kernel_size)]

        self.pre = nn.Sequential(*pre_process)
        self.post = nn.Sequential(*post_precess)

    def forward(self, x1):
        x = self.pre(x1)
        res1 = self.g1(x)
        res2 = self.g2(res1)
        res3 = self.g3(res2)
        w = self.ca(torch.cat([res1, res2, res3], dim=1))
        w = w.view(-1, self.gps, self.dim)[:, :, :, None, None]
        out = w[:, 0, ::] * res1 + w[:, 1, ::] * res2 + w[:, 2, ::] * res3
        out = self.pdu(out)
        x = self.post(out)
        return x + x1


if __name__ == "__main__":
    net = C2PNet(gps=3, blocks=19)
    print(net)

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint as checkpoint
from timm.models.layers import DropPath, to_2tuple, trunc_normal_


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


def window_partition(x, window_size):
    """
    Args:
        x: (B, H, W, C)
        window_size (int): window size

    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    """
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows


def window_reverse(windows, window_size, H, W):
    """
    Args:
        windows: (num_windows*B, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image

    Returns:
        x: (B, H, W, C)
    """
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x


class WindowAttention(nn.Module):
    r""" Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.

    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
        self.register_buffer("relative_position_index", relative_position_index)

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)

        self.proj_drop = nn.Dropout(proj_drop)

        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask=None):
        """
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
        """
        B_, N, C = x.shape
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))

        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
        attn = attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)

        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

    def extra_repr(self) -> str:
        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'

    def flops(self, N):
        # calculate flops for 1 window with token length of N
        flops = 0
        # qkv = self.qkv(x)
        flops += N * self.dim * 3 * self.dim
        # attn = (q @ k.transpose(-2, -1))
        flops += self.num_heads * N * (self.dim // self.num_heads) * N
        #  x = (attn @ v)
        flops += self.num_heads * N * N * (self.dim // self.num_heads)
        # x = self.proj(x)
        flops += N * self.dim * self.dim
        return flops


class SwinTransformerBlock(nn.Module):
    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        if min(self.input_resolution) <= self.window_size:
            # if window size is larger than input resolution, we don't partition windows
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        assert 0 <= self.shift_size < self.window_size, "shift_size must in 0-window_size"

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        if self.shift_size > 0:
            attn_mask = self.calculate_mask(self.input_resolution)
        else:
            attn_mask = None

        self.register_buffer("attn_mask", attn_mask)

    def calculate_mask(self, x_size):
        # calculate attention mask for SW-MSA
        H, W = x_size
        img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
        h_slices = (slice(0, -self.window_size),
                    slice(-self.window_size, -self.shift_size),
                    slice(-self.shift_size, None))
        w_slices = (slice(0, -self.window_size),
                    slice(-self.window_size, -self.shift_size),
                    slice(-self.shift_size, None))
        cnt = 0
        for h in h_slices:
            for w in w_slices:
                img_mask[:, h, w, :] = cnt
                cnt += 1

        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))

        return attn_mask

    def forward(self, x, x_size):
        H, W = x_size
        B, L, C = x.shape
        # assert L == H * W, "input feature has wrong size"

        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, C)

        # cyclic shift
        if self.shift_size > 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
        else:
            shifted_x = x

        # partition windows
        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C

        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size
        if self.input_resolution == x_size:
            attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C
        else:
            attn_windows = self.attn(x_windows, mask=self.calculate_mask(x_size).to(x.device))

        # merge windows
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C

        # reverse cyclic shift
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        x = x.view(B, H * W, C)

        # FFN
        x = shortcut + self.drop_path(x)
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, "
               f"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}"

    def flops(self):
        flops = 0
        H, W = self.input_resolution
        # norm1
        flops += self.dim * H * W
        # W-MSA/SW-MSA
        nW = H * W / self.window_size / self.window_size
        flops += nW * self.attn.flops(self.window_size * self.window_size)
        # mlp
        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio
        # norm2
        flops += self.dim * H * W
        return flops


class PatchMerging(nn.Module):
    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(4 * dim)

    def forward(self, x):
        """
        x: B, H*W, C
        """
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"
        assert H % 2 == 0 and W % 2 == 0, f"x size ({H}*{W}) are not even."

        x = x.view(B, H, W, C)

        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.norm(x)
        x = self.reduction(x)

        return x

    def extra_repr(self) -> str:
        return f"input_resolution={self.input_resolution}, dim={self.dim}"

    def flops(self):
        H, W = self.input_resolution
        flops = H * W * self.dim
        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim
        return flops


class BasicLayer(nn.Module):
    def __init__(self, dim, input_resolution, depth, num_heads, window_size,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):

        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.depth = depth
        self.use_checkpoint = use_checkpoint

        # build blocks
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,
                                 num_heads=num_heads, window_size=window_size,
                                 shift_size=0 if (i % 2 == 0) else window_size // 2,
                                 mlp_ratio=mlp_ratio,
                                 qkv_bias=qkv_bias, qk_scale=qk_scale,
                                 drop=drop, attn_drop=attn_drop,
                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                                 norm_layer=norm_layer)
            for i in range(depth)])

        # patch merging layer
        if downsample is not None:
            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)
        else:
            self.downsample = None

    def forward(self, x, x_size):
        for blk in self.blocks:
            if self.use_checkpoint:
                x = checkpoint.checkpoint(blk, x, x_size)
            else:
                x = blk(x, x_size)
        if self.downsample is not None:
            x = self.downsample(x)
        return x

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}"

    def flops(self):
        flops = 0
        for blk in self.blocks:
            flops += blk.flops()
        if self.downsample is not None:
            flops += self.downsample.flops()
        return flops


class RSTB(nn.Module):
    def __init__(self, dim, input_resolution, depth, num_heads, window_size,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,
                 img_size=224, patch_size=4, resi_connection='1conv'):
        super(RSTB, self).__init__()

        self.dim = dim
        self.input_resolution = input_resolution

        self.residual_group = BasicLayer(dim=dim,
                                         input_resolution=input_resolution,
                                         depth=depth,
                                         num_heads=num_heads,
                                         window_size=window_size,
                                         mlp_ratio=mlp_ratio,
                                         qkv_bias=qkv_bias, qk_scale=qk_scale,
                                         drop=drop, attn_drop=attn_drop,
                                         drop_path=drop_path,
                                         norm_layer=norm_layer,
                                         downsample=downsample,
                                         use_checkpoint=use_checkpoint)

        if resi_connection == '1conv':
            self.conv = nn.Conv2d(dim, dim, 3, 1, 1)
        elif resi_connection == '3conv':
            # to save parameters and memory
            self.conv = nn.Sequential(nn.Conv2d(dim, dim // 4, 3, 1, 1), nn.LeakyReLU(negative_slope=0.2, inplace=True),
                                      nn.Conv2d(dim // 4, dim // 4, 1, 1, 0),
                                      nn.LeakyReLU(negative_slope=0.2, inplace=True),
                                      nn.Conv2d(dim // 4, dim, 3, 1, 1))

        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,
            norm_layer=None)

        self.patch_unembed = PatchUnEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,
            norm_layer=None)

    def forward(self, x, x_size):
        # with torch.backends.cudnn.flags(enabled=False):
        return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))) + x

    def flops(self):
        flops = 0
        flops += self.residual_group.flops()
        H, W = self.input_resolution
        flops += H * W * self.dim * self.dim * 9
        flops += self.patch_embed.flops()
        flops += self.patch_unembed.flops()

        return flops


class PatchEmbed(nn.Module):
    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        x = x.flatten(2).transpose(1, 2)  # B Ph*Pw C
        if self.norm is not None:
            x = self.norm(x)
        return x

    def flops(self):
        flops = 0
        H, W = self.img_size
        if self.norm is not None:
            flops += H * W * self.embed_dim
        return flops


class PatchUnEmbed(nn.Module):
    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]

        self.in_chans = in_chans
        self.embed_dim = embed_dim

    def forward(self, x, x_size):
        B, HW, C = x.shape
        x = x.transpose(1, 2).view(B, self.embed_dim, x_size[0], x_size[1])  # B Ph*Pw C
        return x

    def flops(self):
        flops = 0
        return flops


class Upsample(nn.Sequential):
    def __init__(self, scale, num_feat):
        m = []
        if (scale & (scale - 1)) == 0:  # scale = 2^n
            for _ in range(int(math.log(scale, 2))):
                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))
                m.append(nn.PixelShuffle(2))
        elif scale == 3:
            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))
            m.append(nn.PixelShuffle(3))
        else:
            raise ValueError(f'scale {scale} is not supported. ' 'Supported scales: 2^n and 3.')
        super(Upsample, self).__init__(*m)


class UpsampleOneStep(nn.Sequential):
    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):
        self.num_feat = num_feat
        self.input_resolution = input_resolution
        m = []
        m.append(nn.Conv2d(num_feat, (scale ** 2) * num_out_ch, 3, 1, 1))
        m.append(nn.PixelShuffle(scale))
        super(UpsampleOneStep, self).__init__(*m)

    def flops(self):
        H, W = self.input_resolution
        flops = H * W * self.num_feat * 3 * 9
        return flops


class SwinIR(nn.Module):
    def __init__(self, img_size=64, patch_size=1, in_chans=3,
                 embed_dim=96, depths=[6, 6, 6, 6], num_heads=[6, 6, 6, 6],
                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, upscale=2, img_range=1., upsampler='', resi_connection='1conv',
                 **kwargs):
        super(SwinIR, self).__init__()
        num_in_ch = in_chans
        num_out_ch = in_chans
        num_feat = 64
        self.img_range = img_range
        if in_chans == 3:
            rgb_mean = (0.4488, 0.4371, 0.4040)
            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)
        else:
            self.mean = torch.zeros(1, 1, 1, 1)
        self.upscale = upscale
        self.upsampler = upsampler
        self.window_size = window_size

        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)

        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.ape = ape
        self.patch_norm = patch_norm
        self.num_features = embed_dim
        self.mlp_ratio = mlp_ratio

        # split image into non-overlapping patches
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)
        num_patches = self.patch_embed.num_patches
        patches_resolution = self.patch_embed.patches_resolution
        self.patches_resolution = patches_resolution

        # merge non-overlapping patches into image
        self.patch_unembed = PatchUnEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)

        # absolute position embedding
        if self.ape:
            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
            trunc_normal_(self.absolute_pos_embed, std=.02)

        self.pos_drop = nn.Dropout(p=drop_rate)

        # stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

        # build Residual Swin Transformer blocks (RSTB)
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = RSTB(dim=embed_dim,
                         input_resolution=(patches_resolution[0],
                                           patches_resolution[1]),
                         depth=depths[i_layer],
                         num_heads=num_heads[i_layer],
                         window_size=window_size,
                         mlp_ratio=self.mlp_ratio,
                         qkv_bias=qkv_bias, qk_scale=qk_scale,
                         drop=drop_rate, attn_drop=attn_drop_rate,
                         drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],  # no impact on SR results
                         norm_layer=norm_layer,
                         downsample=None,
                         use_checkpoint=use_checkpoint,
                         img_size=img_size,
                         patch_size=patch_size,
                         resi_connection=resi_connection

                         )
            self.layers.append(layer)
        self.norm = norm_layer(self.num_features)

        # build the last conv layer in deep feature extraction
        if resi_connection == '1conv':
            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)
        elif resi_connection == '3conv':
            # to save parameters and memory
            self.conv_after_body = nn.Sequential(nn.Conv2d(embed_dim, embed_dim // 4, 3, 1, 1),
                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),
                                                 nn.Conv2d(embed_dim // 4, embed_dim // 4, 1, 1, 0),
                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),
                                                 nn.Conv2d(embed_dim // 4, embed_dim, 3, 1, 1))

        if self.upsampler == 'pixelshuffle':
            # for classical SR
            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),
                                                      nn.LeakyReLU(inplace=True))
            self.upsample = Upsample(upscale, num_feat)
            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)
        elif self.upsampler == 'pixelshuffledirect':
            # for lightweight SR (to save parameters)
            self.upsample = UpsampleOneStep(upscale, embed_dim, num_out_ch,
                                            (patches_resolution[0], patches_resolution[1]))
        elif self.upsampler == 'nearest+conv':
            # for real-world SR (less artifacts)
            assert self.upscale == 4, 'only support x4 now.'
            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),
                                                      nn.LeakyReLU(inplace=True))
            self.conv_up1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)
            self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)
            self.conv_hr = nn.Conv2d(num_feat, num_feat, 3, 1, 1)
            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)
            self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)
        else:
            # for image denoising and JPEG compression artifact reduction
            self.conv_last = nn.Conv2d(embed_dim, num_out_ch, 3, 1, 1)

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'absolute_pos_embed'}

    @torch.jit.ignore
    def no_weight_decay_keywords(self):
        return {'relative_position_bias_table'}

    def check_image_size(self, x):
        _, _, h, w = x.size()
        mod_pad_h = (self.window_size - h % self.window_size) % self.window_size
        mod_pad_w = (self.window_size - w % self.window_size) % self.window_size
        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')
        return x

    def forward_features(self, x):
        x_size = (x.shape[2], x.shape[3])
        x = self.patch_embed(x)
        if self.ape:
            x = x + self.absolute_pos_embed
        x = self.pos_drop(x)

        for layer in self.layers:
            x = layer(x, x_size)

        x = self.norm(x)  # B L C
        x = self.patch_unembed(x, x_size)

        return x

    def forward(self, x):
        H, W = x.shape[2:]
        x = self.check_image_size(x)

        self.mean = self.mean.type_as(x)
        x = (x - self.mean) * self.img_range

        if self.upsampler == 'pixelshuffle':
            # for classical SR
            x = self.conv_first(x)
            x = self.conv_after_body(self.forward_features(x)) + x
            x = self.conv_before_upsample(x)
            x = self.conv_last(self.upsample(x))
        elif self.upsampler == 'pixelshuffledirect':
            # for lightweight SR
            x = self.conv_first(x)
            x = self.conv_after_body(self.forward_features(x)) + x
            x = self.upsample(x)
        elif self.upsampler == 'nearest+conv':
            # for real-world SR
            x = self.conv_first(x)
            x = self.conv_after_body(self.forward_features(x)) + x
            x = self.conv_before_upsample(x)
            x = self.lrelu(self.conv_up1(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))
            x = self.lrelu(self.conv_up2(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))
            x = self.conv_last(self.lrelu(self.conv_hr(x)))
        else:
            # for image denoising and JPEG compression artifact reduction
            x_first = self.conv_first(x)
            res = self.conv_after_body(self.forward_features(x_first)) + x_first
            x = x + self.conv_last(res)

        x = x / self.img_range + self.mean

        return x[:, :, :H*self.upscale, :W*self.upscale]

    def flops(self):
        flops = 0
        H, W = self.patches_resolution
        flops += H * W * 3 * self.embed_dim * 9
        flops += self.patch_embed.flops()
        for i, layer in enumerate(self.layers):
            flops += layer.flops()
        flops += H * W * 3 * self.embed_dim * self.embed_dim
        flops += self.upsample.flops()
        return flops

if __name__ == '__main__':
    upscale = 4
    window_size = 8
    height = (1024 // upscale // window_size + 1) * window_size
    width = (720 // upscale // window_size + 1) * window_size
    model = SwinIR(upscale=2, img_size=(height, width),
                   window_size=window_size, img_range=1., depths=[6, 6, 6, 6],
                   embed_dim=60, num_heads=[6, 6, 6, 6], mlp_ratio=2, upsampler='pixelshuffledirect')
    print(model)
    print(height, width, model.flops() / 1e9)

    x = torch.randn((1, 3, height, width))
    x = model(x)
    print(x.shape)
    
import os.path
import torch
import torch.nn.functional as F
from torch import nn as nn
import numpy as np
import math
from global_variable import MODEL_PATH
from .dcn import ModulatedDeformConvPack, modulated_deform_conv
from .network_swinir import RSTB
from .ridcp_utils import ResBlock, CombineQuantBlock
from .vgg_arch import VGGFeatureExtractor

WEIGHT_PATH = os.path.join(MODEL_PATH, 'RIDCP/weight_for_matching_dehazing_Flickr.pth')


class DCNv2Pack(ModulatedDeformConvPack):
    def forward(self, x, feat):
        out = self.conv_offset(feat)
        o1, o2, mask = torch.chunk(out, 3, dim=1)
        offset = torch.cat((o1, o2), dim=1)
        mask = torch.sigmoid(mask)

        offset_absmean = torch.mean(torch.abs(offset))
        if offset_absmean > 50:
            print(f'Offset abs mean is {offset_absmean}, larger than 50.')

        return modulated_deform_conv(x, offset, mask, self.weight, self.bias, self.stride, self.padding,
                                     self.dilation, self.groups, self.deformable_groups)


class VectorQuantizer(nn.Module):
    def __init__(self, n_e, e_dim, weight_path=WEIGHT_PATH, beta=0.25,
                 LQ_stage=False, use_weight=True, weight_alpha=1.0):
        super().__init__()
        self.n_e = int(n_e)
        self.e_dim = int(e_dim)
        self.LQ_stage = LQ_stage
        self.beta = beta
        self.use_weight = use_weight
        self.weight_alpha = weight_alpha
        if self.use_weight:
            self.weight = nn.Parameter(torch.load(weight_path))
            self.weight.requires_grad = False
        self.embedding = nn.Embedding(self.n_e, self.e_dim)

    def dist(self, x, y):
        if x.shape == y.shape:
            return (x - y) ** 2
        else:
            return torch.sum(x ** 2, dim=1, keepdim=True) +
                torch.sum(y ** 2, dim=1) - 2 *
                torch.matmul(x, y.t())

    def gram_loss(self, x, y):
        b, h, w, c = x.shape
        x = x.reshape(b, h * w, c)
        y = y.reshape(b, h * w, c)

        gmx = x.transpose(1, 2) @ x / (h * w)
        gmy = y.transpose(1, 2) @ y / (h * w)

        return (gmx - gmy).square().mean()

    def forward(self, z, gt_indices=None, current_iter=None, weight_alpha=None):
        """
        Args:
            z: input features to be quantized, z (continuous) -> z_q (discrete)
               z.shape = (batch, channel, height, width)
            gt_indices: feature map of given indices, used for visualization.
        """
        # reshape z -> (batch, height, width, channel) and flatten
        z = z.permute(0, 2, 3, 1).contiguous()
        z_flattened = z.view(-1, self.e_dim)

        codebook = self.embedding.weight

        d = self.dist(z_flattened, codebook)
        if self.use_weight and self.LQ_stage:
            if weight_alpha is not None:
                self.weight_alpha = weight_alpha
            d = d * torch.exp(self.weight_alpha * self.weight)

        # find closest encodings
        min_encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)
        min_encodings = torch.zeros(min_encoding_indices.shape[0], codebook.shape[0]).to(z)
        min_encodings.scatter_(1, min_encoding_indices, 1)

        if gt_indices is not None:
            gt_indices = gt_indices.reshape(-1)

            gt_min_indices = gt_indices.reshape_as(min_encoding_indices)
            gt_min_onehot = torch.zeros(gt_min_indices.shape[0], codebook.shape[0]).to(z)
            gt_min_onehot.scatter_(1, gt_min_indices, 1)

            z_q_gt = torch.matmul(gt_min_onehot, codebook)
            z_q_gt = z_q_gt.view(z.shape)

        # get quantized latent vectors
        z_q = torch.matmul(min_encodings, codebook)
        z_q = z_q.view(z.shape)

        e_latent_loss = torch.mean((z_q.detach() - z) ** 2)
        q_latent_loss = torch.mean((z_q - z.detach()) ** 2)

        if self.LQ_stage and gt_indices is not None:
            # codebook_loss = self.dist(z_q, z_q_gt.detach()).mean() \
            # + self.beta * self.dist(z_q_gt.detach(), z)
            codebook_loss = self.beta * self.dist(z_q_gt.detach(), z)
            texture_loss = self.gram_loss(z, z_q_gt.detach())
            # print("codebook loss:", codebook_loss.mean(), "\ntexture_loss: ", texture_loss.mean())
            codebook_loss = codebook_loss + texture_loss
        else:
            codebook_loss = q_latent_loss + e_latent_loss * self.beta

        # preserve gradients
        z_q = z + (z_q - z).detach()

        # reshape back to match original input shape
        z_q = z_q.permute(0, 3, 1, 2).contiguous()

        return z_q, codebook_loss, min_encoding_indices.reshape(z_q.shape[0], 1, z_q.shape[2], z_q.shape[3])

    def get_codebook_entry(self, indices):
        b, _, h, w = indices.shape

        indices = indices.flatten().to(self.embedding.weight.device)
        min_encodings = torch.zeros(indices.shape[0], self.n_e).to(indices)
        min_encodings.scatter_(1, indices[:, None], 1)

        # get quantized latent vectors
        z_q = torch.matmul(min_encodings.float(), self.embedding.weight)
        z_q = z_q.view(b, h, w, -1).permute(0, 3, 1, 2).contiguous()
        return z_q


class SwinLayers(nn.Module):
    def __init__(self, input_resolution=(32, 32), embed_dim=256,
                 blk_depth=6,
                 num_heads=8,
                 window_size=8,
                 **kwargs):
        super().__init__()
        self.swin_blks = nn.ModuleList()
        for i in range(4):
            layer = RSTB(embed_dim, input_resolution, blk_depth, num_heads, window_size, patch_size=1, **kwargs)
            self.swin_blks.append(layer)

    def forward(self, x):
        b, c, h, w = x.shape
        x = x.reshape(b, c, h * w).transpose(1, 2)
        for m in self.swin_blks:
            x = m(x, (h, w))
        x = x.transpose(1, 2).reshape(b, c, h, w)
        return x


class MultiScaleEncoder(nn.Module):
    def __init__(self,
                 in_channel,
                 max_depth,
                 input_res=256,
                 channel_query_dict=None,
                 norm_type='gn',
                 act_type='leakyrelu',
                 LQ_stage=True,
                 **swin_opts,
                 ):
        super().__init__()
        self.LQ_stage = LQ_stage
        ksz = 3

        self.in_conv = nn.Conv2d(in_channel, channel_query_dict[input_res], 4, padding=1)

        self.blocks = nn.ModuleList()
        self.up_blocks = nn.ModuleList()
        self.max_depth = max_depth
        res = input_res
        for i in range(max_depth):
            in_ch, out_ch = channel_query_dict[res], channel_query_dict[res // 2]
            tmp_down_block = [
                nn.Conv2d(in_ch, out_ch, ksz, stride=2, padding=1),
                ResBlock(out_ch, out_ch, norm_type, act_type),
                ResBlock(out_ch, out_ch, norm_type, act_type),
            ]
            self.blocks.append(nn.Sequential(*tmp_down_block))
            res = res // 2

        if LQ_stage:
            self.blocks.append(SwinLayers(**swin_opts))

    def forward(self, input):
        # input.requires_grad = True
        x = self.in_conv(input)
        for idx, m in enumerate(self.blocks):
            with torch.backends.cudnn.flags(enabled=False):
                x = m(x)
        return x
    
class DecoderBlock(nn.Module):

    def __init__(self, in_channel, out_channel, norm_type='gn', act_type='leakyrelu'):
        super().__init__()

        self.block = []
        self.block += [
            nn.Upsample(scale_factor=2),
            nn.Conv2d(in_channel, out_channel, 3, stride=1, padding=1),
            ResBlock(out_channel, out_channel, norm_type, act_type),
            ResBlock(out_channel, out_channel, norm_type, act_type),
        ]

        self.block = nn.Sequential(*self.block)

    def forward(self, input):
        return self.block(input)


class WarpBlock(nn.Module):
    def __init__(self, in_channel):
        super().__init__()
        self.offset = nn.Conv2d(in_channel * 2, in_channel, 3, stride=1, padding=1)
        self.dcn = DCNv2Pack(in_channel, in_channel, 3, padding=1, deformable_groups=4)

    def forward(self, x_vq, x_residual):
        x_residual = self.offset(torch.cat([x_vq, x_residual], dim=1))
        feat_after_warp = self.dcn(x_vq, x_residual)

        return feat_after_warp


class MultiScaleDecoder(nn.Module):
    def __init__(self,
                 in_channel,
                 max_depth,
                 input_res=256,
                 channel_query_dict=None,
                 norm_type='gn',
                 act_type='leakyrelu',
                 only_residual=False,
                 use_warp=True
                 ):
        super().__init__()
        self.only_residual = only_residual
        self.use_warp = use_warp
        self.upsampler = nn.ModuleList()
        self.warp = nn.ModuleList()
        res = input_res // (2 ** max_depth)
        for i in range(max_depth):
            in_channel, out_channel = channel_query_dict[res], channel_query_dict[res * 2]
            self.upsampler.append(nn.Sequential(
                nn.Upsample(scale_factor=2),
                nn.Conv2d(in_channel, out_channel, 3, stride=1, padding=1),
                ResBlock(out_channel, out_channel, norm_type, act_type),
                ResBlock(out_channel, out_channel, norm_type, act_type),
            )
            )
            self.warp.append(WarpBlock(out_channel))
            res = res * 2

    def forward(self, input, code_decoder_output):
        x = input
        for idx, m in enumerate(self.upsampler):
            with torch.backends.cudnn.flags(enabled=False):
                if not self.only_residual:
                    x = m(x)
                    if self.use_warp:
                        x_vq = self.warp[idx](code_decoder_output[idx], x)
                        # print(idx, x.mean(), x_vq.mean())
                        x = x + x_vq * (x.mean() / x_vq.mean())
                    else:
                        x = x + code_decoder_output[idx]
                else:
                    x = m(x)
        # print()
        return x


class VQWeightDehazeNet(nn.Module):
    def __init__(self,
                 *,
                 in_channel=3,
                 codebook_params=None,
                 gt_resolution=256,
                 LQ_stage=False,
                 norm_type='gn',
                 act_type='silu',
                 use_quantize=True,
                 use_semantic_loss=False,
                 use_residual=True,
                 only_residual=False,
                 use_weight=False,
                 use_warp=True,
                 weight_alpha=1.0,
                 **ignore_kwargs):
        super().__init__()

        codebook_params = np.array(codebook_params)

        self.codebook_scale = codebook_params[:, 0]
        codebook_emb_num = codebook_params[:, 1].astype(int)
        codebook_emb_dim = codebook_params[:, 2].astype(int)

        self.use_quantize = use_quantize
        self.in_channel = in_channel
        self.gt_res = gt_resolution
        self.LQ_stage = LQ_stage
        self.use_residual = use_residual
        self.only_residual = only_residual
        self.use_weight = use_weight
        self.use_warp = use_warp
        self.weight_alpha = weight_alpha

        channel_query_dict = {
            8: 256,
            16: 256,
            32: 256,
            64: 256,
            128: 128,
            256: 64,
            512: 32,
        }

        # build encoder
        self.max_depth = int(np.log2(gt_resolution // self.codebook_scale[0]))
        self.multiscale_encoder = MultiScaleEncoder(
            in_channel,
            self.max_depth,
            self.gt_res,
            channel_query_dict,
            norm_type, act_type, LQ_stage
        )
        if self.LQ_stage and self.use_residual:
            self.multiscale_decoder = MultiScaleDecoder(
                in_channel,
                self.max_depth,
                self.gt_res,
                channel_query_dict,
                norm_type, act_type, only_residual, use_warp=self.use_warp
            )

        # build decoder
        self.decoder_group = nn.ModuleList()
        for i in range(self.max_depth):
            res = gt_resolution // 2 ** self.max_depth * 2 ** i
            in_ch, out_ch = channel_query_dict[res], channel_query_dict[res * 2]
            self.decoder_group.append(DecoderBlock(in_ch, out_ch, norm_type, act_type))

        self.out_conv = nn.Conv2d(out_ch, 3, 3, 1, 1)
        self.residual_conv = nn.Conv2d(out_ch, 3, 3, 1, 1)

        # build multi-scale vector quantizers
        self.quantize_group = nn.ModuleList()
        self.before_quant_group = nn.ModuleList()
        self.after_quant_group = nn.ModuleList()

        for scale in range(0, codebook_params.shape[0]):
            quantize = VectorQuantizer(
                codebook_emb_num[scale],
                codebook_emb_dim[scale],
                LQ_stage=self.LQ_stage,
                use_weight=self.use_weight,
                weight_alpha=self.weight_alpha
            )
            self.quantize_group.append(quantize)

            scale_in_ch = channel_query_dict[self.codebook_scale[scale]]
            if scale == 0:
                quant_conv_in_ch = scale_in_ch
                comb_quant_in_ch1 = codebook_emb_dim[scale]
                comb_quant_in_ch2 = 0
            else:
                quant_conv_in_ch = scale_in_ch * 2
                comb_quant_in_ch1 = codebook_emb_dim[scale - 1]
                comb_quant_in_ch2 = codebook_emb_dim[scale]

            self.before_quant_group.append(nn.Conv2d(quant_conv_in_ch, codebook_emb_dim[scale], 1))
            self.after_quant_group.append(CombineQuantBlock(comb_quant_in_ch1, comb_quant_in_ch2, scale_in_ch))

        # semantic loss for HQ pretrain stage
        self.use_semantic_loss = use_semantic_loss
        if use_semantic_loss:
            self.conv_semantic = nn.Sequential(
                nn.Conv2d(512, 512, 1, 1, 0),
                nn.ReLU(),
            )
            self.vgg_feat_layer = 'relu4_4'
            self.vgg_feat_extractor = VGGFeatureExtractor([self.vgg_feat_layer])

    def encode_and_decode(self, input, gt_indices=None, current_iter=None, weight_alpha=None):
        # if self.training:
        #     for p in self.multiscale_encoder.parameters():
        #         p.requires_grad = True
        enc_feats = self.multiscale_encoder(input)

        if self.use_semantic_loss:
            with torch.no_grad():
                vgg_feat = self.vgg_feat_extractor(input)[self.vgg_feat_layer]

        codebook_loss_list = []
        indices_list = []
        semantic_loss_list = []
        code_decoder_output = []

        quant_idx = 0
        prev_dec_feat = None
        prev_quant_feat = None
        out_img = None
        out_img_residual = None

        x = enc_feats

        for i in range(self.max_depth):
            cur_res = self.gt_res // 2 ** self.max_depth * 2 ** i
            if cur_res in self.codebook_scale:  # needs to perform quantize
                if prev_dec_feat is not None:
                    before_quant_feat = torch.cat((x, prev_dec_feat), dim=1)
                else:
                    before_quant_feat = x
                feat_to_quant = self.before_quant_group[quant_idx](before_quant_feat)

                if weight_alpha is not None:
                    self.weight_alpha = weight_alpha
                if gt_indices is not None:
                    z_quant, codebook_loss, indices = self.quantize_group[quant_idx](feat_to_quant,
                                                                                     gt_indices[quant_idx],
                                                                                     weight_alpha=self.weight_alpha)
                else:
                    z_quant, codebook_loss, indices = self.quantize_group[quant_idx](feat_to_quant,
                                                                                     weight_alpha=self.weight_alpha)

                if self.use_semantic_loss:
                    semantic_z_quant = self.conv_semantic(z_quant)
                    semantic_loss = F.mse_loss(semantic_z_quant, vgg_feat)
                    semantic_loss_list.append(semantic_loss)

                if not self.use_quantize:
                    z_quant = feat_to_quant

                after_quant_feat = self.after_quant_group[quant_idx](z_quant, prev_quant_feat)

                codebook_loss_list.append(codebook_loss)
                indices_list.append(indices)

                quant_idx += 1
                prev_quant_feat = z_quant
                x = after_quant_feat

            x = self.decoder_group[i](x)
            code_decoder_output.append(x)
            prev_dec_feat = x

        out_img = self.out_conv(x)

        if self.LQ_stage and self.use_residual:
            if self.only_residual:
                residual_feature = self.multiscale_decoder(enc_feats, code_decoder_output)
            else:
                residual_feature = self.multiscale_decoder(enc_feats.detach(), code_decoder_output)
            out_img_residual = self.residual_conv(residual_feature)

        if len(codebook_loss_list) > 0:
            codebook_loss = sum(codebook_loss_list)
        else:
            codebook_loss = 0
        semantic_loss = sum(semantic_loss_list) if len(semantic_loss_list) else codebook_loss * 0

        return out_img, out_img_residual, codebook_loss, semantic_loss, feat_to_quant, z_quant, indices_list

    def decode_indices(self, indices):
        assert len(indices.shape) == 4, f'shape of indices must be (b, 1, h, w), but got {indices.shape}'

        z_quant = self.quantize_group[0].get_codebook_entry(indices)
        x = self.after_quant_group[0](z_quant)

        for m in self.decoder_group:
            x = m(x)
        out_img = self.out_conv(x)
        return out_img

    @torch.no_grad()
    def test_tile(self, input, tile_size=240, tile_pad=16):
        batch, channel, height, width = input.shape
        output_height = height
        output_width = width
        output_shape = (batch, channel, output_height, output_width)

        # start with black image
        output = input.new_zeros(output_shape)
        tiles_x = math.ceil(width / tile_size)
        tiles_y = math.ceil(height / tile_size)

        # loop over all tiles
        for y in range(tiles_y):
            for x in range(tiles_x):
                # extract tile from input image
                ofs_x = x * tile_size
                ofs_y = y * tile_size
                # input tile area on total image
                input_start_x = ofs_x
                input_end_x = min(ofs_x + tile_size, width)
                input_start_y = ofs_y
                input_end_y = min(ofs_y + tile_size, height)

                # input tile area on total image with padding
                input_start_x_pad = max(input_start_x - tile_pad, 0)
                input_end_x_pad = min(input_end_x + tile_pad, width)
                input_start_y_pad = max(input_start_y - tile_pad, 0)
                input_end_y_pad = min(input_end_y + tile_pad, height)

                # input tile dimensions
                input_tile_width = input_end_x - input_start_x
                input_tile_height = input_end_y - input_start_y
                tile_idx = y * tiles_x + x + 1
                input_tile = input[:, :, input_start_y_pad:input_end_y_pad, input_start_x_pad:input_end_x_pad]

                # upscale tile
                output_tile = self.test(input_tile)

                # output tile area on total image
                output_start_x = input_start_x
                output_end_x = input_end_x
                output_start_y = input_start_y
                output_end_y = input_end_y

                # output tile area without padding
                output_start_x_tile = (input_start_x - input_start_x_pad)
                output_end_x_tile = output_start_x_tile + input_tile_width
                output_start_y_tile = (input_start_y - input_start_y_pad)
                output_end_y_tile = output_start_y_tile + input_tile_height

                # put tile into output image
                output[:, :, output_start_y:output_end_y,
                output_start_x:output_end_x] = output_tile[:, :, output_start_y_tile:output_end_y_tile,
                                               output_start_x_tile:output_end_x_tile]
        return output

    @torch.no_grad()
    def test(self, input, weight_alpha=None):
        org_use_semantic_loss = self.use_semantic_loss
        self.use_semantic_loss = False

        # padding to multiple of window_size * 8
        wsz = 32
        _, _, h_old, w_old = input.shape
        h_pad = (h_old // wsz + 1) * wsz - h_old
        w_pad = (w_old // wsz + 1) * wsz - w_old
        input = torch.cat([input, torch.flip(input, [2])], 2)[:, :, :h_old + h_pad, :]
        input = torch.cat([input, torch.flip(input, [3])], 3)[:, :, :, :w_old + w_pad]

        output_vq, output, _, _, _, after_quant, index = self.encode_and_decode(input, None, None,
                                                                                weight_alpha=weight_alpha)

        if output is not None:
            output = output[..., :h_old, :w_old]
        if output_vq is not None:
            output_vq = output_vq[..., :h_old, :w_old]

        self.use_semantic_loss = org_use_semantic_loss
        return output, index

    def forward(self, input, gt_indices=None, weight_alpha=None):
        if gt_indices is not None:
            # in LQ training stage, need to pass GT indices for supervise.
            dec, dec_residual, codebook_loss, semantic_loss, quant_before_feature, quant_after_feature, indices = self.encode_and_decode(
                input, gt_indices, weight_alpha=weight_alpha)
        else:
            # in HQ stage, or LQ test stage, no GT indices needed.
            dec, dec_residual, codebook_loss, semantic_loss, quant_before_feature, quant_after_feature, indices = self.encode_and_decode(
                input, weight_alpha=weight_alpha)
        return dec, dec_residual, codebook_loss, semantic_loss, quant_before_feature, quant_after_feature, indices
```
